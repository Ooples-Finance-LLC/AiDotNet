using AiDotNet.Compression.Quantization;
using AiDotNet.Compression.Pruning;
using AiDotNet.Interfaces;
using AiDotNet.Enums;
using AiDotNet.Helpers;
using System;
using System.Collections.Generic;
using System.IO;

namespace AiDotNet.Compression.Hardware;

/// <summary>
/// Provides hardware-specific optimizations for compressed model inference.
/// </summary>
/// <remarks>
/// <para>
/// This class applies hardware-specific optimizations to compressed models to enhance
/// inference performance on different types of hardware.
/// </para>
/// <para><b>For Beginners:</b> This makes compressed models run faster on specific hardware.
/// 
/// Different devices have different capabilities:
/// - Some CPUs have special instructions for int8 operations
/// - Some GPUs are optimized for certain types of operations
/// - Mobile devices have specific hardware accelerators
/// 
/// This class helps take advantage of these hardware features to make
/// your compressed models run as efficiently as possible.
/// </para>
/// </remarks>
/// <typeparam name="TModel">The type of model to optimize.</typeparam>
/// <typeparam name="TInput">The input type for the model.</typeparam>
/// <typeparam name="TOutput">The output type for the model.</typeparam>
public class InferenceOptimizer<TModel, TInput, TOutput> 
    : ModelCompressorBase<TModel, TInput, TOutput>
    where TModel : class, IFullModel<double, TInput, TOutput>
{
    /// <summary>
    /// The detected hardware capabilities.
    /// </summary>
    private readonly HardwareCapabilities _capabilities;
    
    /// <summary>
    /// Whether SIMD optimizations are available.
    /// </summary>
    private readonly bool _hasSIMD;
    
    /// <summary>
    /// Whether hardware-specific optimizations have been applied.
    /// </summary>
    private bool _optimizationsApplied;
    
    /// <summary>
    /// Creates a new compressor with the specified options.
    /// </summary>
    /// <param name="options">The compression options to use.</param>
    /// <returns>A new compressor instance with the specified options.</returns>
    protected override IModelCompressor<TModel, TInput, TOutput> CreateCompressorWithOptions(ModelCompressionOptions options)
    {
        // Since this class doesn't actually create new compressors (it just optimizes existing ones),
        // simply return a new instance of the same class with the same options
        return new InferenceOptimizer<TModel, TInput, TOutput>(DetectHardwareCapabilities(), options);
    }
    
    /// <summary>
    /// Initializes a new instance of the <see cref="InferenceOptimizer{TModel, TInput, TOutput}"/> class.
    /// </summary>
    /// <remarks>
    /// <para>
    /// This constructor detects the available hardware capabilities.
    /// </para>
    /// <para><b>For Beginners:</b> This automatically figures out what your hardware can do.
    /// 
    /// When created, the optimizer:
    /// - Checks what CPU and instructions are available
    /// - Determines if GPU acceleration is available
    /// - Identifies any special hardware accelerators
    /// 
    /// This helps it apply the right optimizations for your specific device.
    /// </para>
    /// </remarks>
    public InferenceOptimizer() : base(new ModelCompressionOptions 
        { 
            Technique = Enums.CompressionTechnique.None
        })
    {
        _capabilities = DetectHardwareCapabilities();
        _hasSIMD = MathHelper.IsHardwareAccelerated;
    }
    
    /// <summary>
    /// Initializes a new instance of the <see cref="InferenceOptimizer{TModel, TInput, TOutput}"/> class with specified capabilities.
    /// </summary>
    /// <param name="capabilities">The hardware capabilities to use.</param>
    /// <param name="options">The options for model compression.</param>
    /// <remarks>
    /// <para>
    /// This constructor allows manually specifying the hardware capabilities to use.
    /// </para>
    /// <para><b>For Beginners:</b> This lets you manually specify what hardware features to use.
    /// 
    /// You might use this when:
    /// - You want to target specific hardware that may not be the current device
    /// - You want to turn on or off specific optimizations
    /// - You're testing performance across different hardware profiles
    /// </para>
    /// </remarks>
    public InferenceOptimizer(HardwareCapabilities capabilities, ModelCompressionOptions? options = null) 
        : base(options ?? new ModelCompressionOptions 
        { 
            Technique = Enums.CompressionTechnique.None
        })
    {
        _capabilities = capabilities;
        _hasSIMD = capabilities.HasSIMD;
    }
    
    /// <summary>
    /// Optimizes a compressed model for the detected hardware.
    /// </summary>
    /// <param name="model">The compressed model to optimize.</param>
    /// <returns>The hardware-optimized model.</returns>
    /// <remarks>
    /// <para>
    /// This method applies hardware-specific optimizations to a compressed model.
    /// </para>
    /// <para><b>For Beginners:</b> This tunes your compressed model for your specific hardware.
    /// 
    /// The optimization process:
    /// 1. Identifies what compression technique was used (quantization, pruning, etc.)
    /// 2. Applies hardware-specific optimizations for that technique
    /// 3. Returns a model that will run faster on your specific hardware
    /// 
    /// This can significantly improve inference speed without changing the model's accuracy.
    /// </para>
    /// </remarks>
    public TModel OptimizeForHardware(TModel model)
    {
        // Check what kind of compressed model we're dealing with
        if (model is IQuantizedModel<double, TInput, TOutput> quantizedModel)
        {
            return OptimizeQuantizedModel(model, quantizedModel);
        }
        else if (model is IPrunedModel<double, TInput, TOutput> prunedModel)
        {
            return OptimizePrunedModel(model, prunedModel);
        }
        else if (model is IDistilledModel<double, TInput, TOutput>)
        {
            // Distilled models typically don't need special hardware optimizations
            // beyond what's already applied to the model itself
            return model;
        }
        
        // If it's not a recognized compressed model, return it unchanged
        return model;
    }
    
    /// <summary>
    /// Gets information about the applied optimizations.
    /// </summary>
    /// <returns>A dictionary of applied optimizations.</returns>
    /// <remarks>
    /// <para>
    /// This method returns information about which optimizations were applied to the model.
    /// </para>
    /// <para><b>For Beginners:</b> This tells you what optimizations were actually applied.
    /// 
    /// The dictionary includes:
    /// - Which hardware features were detected
    /// - Which optimizations were successfully applied
    /// - Any fallbacks that were used
    /// 
    /// This helps you understand how your model is being accelerated.
    /// </para>
    /// </remarks>
    public Dictionary<string, object> GetAppliedOptimizations()
    {
        if (!_optimizationsApplied)
        {
            return new Dictionary<string, object>
            {
                { "OptimizationsApplied", false },
                { "Reason", "No model has been optimized yet" }
            };
        }
        
        var optimizations = new Dictionary<string, object>
        {
            { "OptimizationsApplied", true },
            { "HardwareCapabilities", _capabilities },
            { "SIMD", _hasSIMD }
        };
        
        if (_capabilities.HasAvx2)
        {
            optimizations["AVX2Optimizations"] = true;
        }
        
        if (_capabilities.HasInt8Acceleration)
        {
            optimizations["Int8Acceleration"] = true;
        }
        
        if (_capabilities.HasGPU)
        {
            optimizations["GPUAcceleration"] = true;
            optimizations["GPUType"] = _capabilities.GPUType;
        }
        
        if (_capabilities.HasNPU)
        {
            optimizations["NPUAcceleration"] = true;
            optimizations["NPUType"] = _capabilities.NPUType;
        }
        
        return optimizations;
    }
    
    /// <summary>
    /// Optimizes a quantized model for the detected hardware.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    /// <param name="quantizedModel">The model as an IQuantizedModel.</param>
    /// <returns>The hardware-optimized model.</returns>
    /// <remarks>
    /// <para>
    /// This method applies hardware-specific optimizations to a quantized model.
    /// </para>
    /// <para><b>For Beginners:</b> This optimizes quantized models for your hardware.
    /// 
    /// For quantized models, this might:
    /// - Use special CPU instructions for int8 math
    /// - Organize memory for better cache usage
    /// - Use hardware-specific dequantization
    /// - Leverage tensor cores on compatible GPUs
    /// </para>
    /// </remarks>
    private TModel OptimizeQuantizedModel(TModel model, IQuantizedModel<double, TInput, TOutput> quantizedModel)
    {
        _optimizationsApplied = true;
        
        // Check if the model supports hardware-specific optimizations
        if (model is IHardwareOptimizable<TModel, TInput, TOutput> optimizableModel)
        {
            // Let the model apply its own optimizations based on capabilities
            return optimizableModel.OptimizeForHardware(_capabilities);
        }
        
        // Apply standard optimizations
        
        // 1. Memory layout optimizations for better cache locality
        AlignMemoryLayout(model);
        
        // 2. Hardware-specific optimizations based on capabilities
        if (_capabilities.HasInt8Acceleration && quantizedModel.QuantizationBitWidth <= 8)
        {
            // Apply int8 optimizations when available
            ApplyInt8Optimizations(model);
        }
        
        if (_capabilities.HasAvx2 && quantizedModel.QuantizationBitWidth == 8)
        {
            // Apply AVX2 optimizations for 8-bit quantized models
            ApplyAvx2Optimizations(model);
        }
        
        if (_capabilities.HasGPU)
        {
            // Apply GPU optimizations if available and beneficial
            if (IsGpuBeneficial(model))
            {
                ApplyGpuOptimizations(model);
            }
        }
        
        if (_capabilities.HasNPU)
        {
            // Apply Neural Processing Unit optimizations if available
            ApplyNpuOptimizations(model);
        }
        
        return model;
    }
    
    /// <summary>
    /// Optimizes a pruned model for the detected hardware.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    /// <param name="prunedModel">The model as an IPrunedModel.</param>
    /// <returns>The hardware-optimized model.</returns>
    /// <remarks>
    /// <para>
    /// This method applies hardware-specific optimizations to a pruned model.
    /// </para>
    /// <para><b>For Beginners:</b> This optimizes pruned models for your hardware.
    /// 
    /// For pruned models, this might:
    /// - Use sparse matrix operations when supported
    /// - Optimize memory layout for sparse computations
    /// - Use hardware-specific sparse acceleration if available
    /// </para>
    /// </remarks>
    private TModel OptimizePrunedModel(TModel model, IPrunedModel<double, TInput, TOutput> prunedModel)
    {
        _optimizationsApplied = true;
        
        // Check if the model supports hardware-specific optimizations
        if (model is IHardwareOptimizable<TModel, TInput, TOutput> optimizableModel)
        {
            // Let the model apply its own optimizations based on capabilities
            return optimizableModel.OptimizeForHardware(_capabilities);
        }
        
        // Apply standard optimizations for pruned models
        
        // 1. Convert to optimal sparse format for the hardware
        OptimizeSparseFormat(model, prunedModel);
        
        // 2. Apply hardware-specific sparse optimizations
        if (_capabilities.HasSparseAcceleration)
        {
            ApplySparseOptimizations(model);
        }
        
        if (_capabilities.HasGPU && _capabilities.GPUType == "NVIDIA" && prunedModel.SparsityLevel >= 0.5)
        {
            // NVIDIA GPUs have specific sparse matrix acceleration
            ApplyNvidiaSparseOptimizations(model);
        }
        
        return model;
    }
    
    /// <summary>
    /// Detects the hardware capabilities of the current system.
    /// </summary>
    /// <returns>A HardwareCapabilities object describing the system capabilities.</returns>
    /// <remarks>
    /// <para>
    /// This method detects the available hardware capabilities to determine which
    /// optimizations can be applied.
    /// </para>
    /// <para><b>For Beginners:</b> This figures out what your hardware can do.
    /// 
    /// The detection process checks:
    /// - CPU features like AVX, AVX2, and special instructions
    /// - Whether a GPU is available and what type
    /// - Whether special accelerators like NPUs are available
    /// - What memory architecture is being used
    /// 
    /// This information helps select the right optimizations.
    /// </para>
    /// </remarks>
    private HardwareCapabilities DetectHardwareCapabilities()
    {
        var capabilities = new HardwareCapabilities();
        
        // CPU capabilities detection
        capabilities.HasSIMD = MathHelper.IsHardwareAccelerated;
        
        // Check for AVX2 support
        capabilities.HasAvx2 = IsAvx2Supported();
        
        // Check for int8 acceleration support
        capabilities.HasInt8Acceleration = IsInt8AccelerationSupported();
        
        // Check for sparse acceleration
        capabilities.HasSparseAcceleration = IsSparseAccelerationSupported();
        
        // GPU detection
        DetectGpu(capabilities);
        
        // NPU detection (Neural Processing Unit)
        DetectNpu(capabilities);
        
        return capabilities;
    }
    
    /// <summary>
    /// Determines if AVX2 instructions are supported by the CPU.
    /// </summary>
    /// <returns>True if AVX2 is supported, otherwise false.</returns>
    private bool IsAvx2Supported()
    {
        try
        {
            // In a real implementation, this would use CPU feature detection
            // For .NET Framework 4.6.2, we'll check if we're on a 64-bit process
            if (Environment.Is64BitProcess)
            {
                // Most modern x64 processors support AVX2, but a proper check
                // would use platform-specific CPU feature detection
                return true;
            }
            
            return false;
        }
        catch
        {
            // If detection fails, assume it's not supported
            return false;
        }
    }
    
    /// <summary>
    /// Determines if int8 acceleration is supported by the hardware.
    /// </summary>
    /// <returns>True if int8 acceleration is supported, otherwise false.</returns>
    private bool IsInt8AccelerationSupported()
    {
        try
        {
            // In a real implementation, this would check for specific CPU features
            // like SSE4.1, AVX2, or ARM NEON that accelerate int8 operations
            
            // For .NET Framework 4.6.2, we'll use Environment properties
            if (Environment.Is64BitProcess)
            {
                // Most 64-bit processors have int8 acceleration
                return true;
            }
            
            // Check if we're on ARM by examining processor information
            var processorArchitecture = Environment.GetEnvironmentVariable("PROCESSOR_ARCHITECTURE");
            if (processorArchitecture != null && processorArchitecture.Contains("ARM"))
            {
                // ARM typically has good int8 support via NEON
                return true;
            }
            
            return false;
        }
        catch
        {
            // If detection fails, assume it's not supported
            return false;
        }
    }
    
    /// <summary>
    /// Determines if sparse matrix acceleration is supported by the hardware.
    /// </summary>
    /// <returns>True if sparse acceleration is supported, otherwise false.</returns>
    private bool IsSparseAccelerationSupported()
    {
        // Most modern CPUs and GPUs have some level of sparse matrix support,
        // but it varies widely by hardware. In a real implementation, this would
        // check for specific hardware features or libraries
        
        // For .NET Framework 4.6.2, we'll assume it's supported on 64-bit processes
        return Environment.Is64BitProcess;
    }
    
    /// <summary>
    /// Detects GPU availability and type.
    /// </summary>
    /// <param name="capabilities">The hardware capabilities to update.</param>
    private void DetectGpu(HardwareCapabilities capabilities)
    {
        try
        {
            // In a real implementation, this would use platform-specific GPU detection
            // For this example, we'll simulate detection
            
            // Look for NVIDIA GPU using environment variables
            if (Environment.GetEnvironmentVariable("CUDA_VISIBLE_DEVICES") != null)
            {
                capabilities.HasGPU = true;
                capabilities.GPUType = "NVIDIA";
                return;
            }
            
            // Look for AMD GPU
            if (Environment.GetEnvironmentVariable("HSA_OVERRIDE_GFX_VERSION") != null)
            {
                capabilities.HasGPU = true;
                capabilities.GPUType = "AMD";
                return;
            }
            
            // A more sophisticated implementation would check for other GPU types
            // and use proper GPU detection libraries
            
            capabilities.HasGPU = false;
            capabilities.GPUType = "None";
        }
        catch
        {
            // If detection fails, assume no GPU
            capabilities.HasGPU = false;
            capabilities.GPUType = "None";
        }
    }
    
    /// <summary>
    /// Detects NPU (Neural Processing Unit) availability and type.
    /// </summary>
    /// <param name="capabilities">The hardware capabilities to update.</param>
    private void DetectNpu(HardwareCapabilities capabilities)
    {
        try
        {
            // In a real implementation, this would check for various NPUs
            // such as Apple Neural Engine, Google Edge TPU, etc.
            
            // For .NET Framework 4.6.2, we'll check the OS platform
            var platform = Environment.OSVersion.Platform;
            var osVersion = Environment.OSVersion.VersionString;
            
            // Check if we're on macOS (which might have Apple Neural Engine)
            if (platform == PlatformID.Unix && osVersion.Contains("Darwin"))
            {
                // Apple devices might have Neural Engine
                capabilities.HasNPU = true;
                capabilities.NPUType = "Apple Neural Engine";
                return;
            }
            
            // A more sophisticated implementation would check for other NPU types
            
            capabilities.HasNPU = false;
            capabilities.NPUType = "None";
        }
        catch
        {
            // If detection fails, assume no NPU
            capabilities.HasNPU = false;
            capabilities.NPUType = "None";
        }
    }
    
    /// <summary>
    /// Aligns memory layout for better cache locality.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    private void AlignMemoryLayout(TModel model)
    {
        // In a real implementation, this would reorganize model parameters
        // for better memory access patterns and cache utilization
        
        // For example, it might:
        // - Ensure parameters are aligned to cache line boundaries
        // - Group frequently accessed parameters together
        // - Convert memory layout to be more efficient for SIMD operations
        
        // This is a placeholder for the actual implementation
    }
    
    /// <summary>
    /// Applies int8 optimizations to a model.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    private void ApplyInt8Optimizations(TModel model)
    {
        // In a real implementation, this would apply specific int8 optimizations
        // based on the hardware capabilities
        
        // For example, it might:
        // - Set up optimized int8 matrix multiplication routines
        // - Configure hardware-specific dequantization
        // - Use specialized int8 instructions available on the CPU
        
        // This is a placeholder for the actual implementation
    }
    
    /// <summary>
    /// Applies AVX2 optimizations to a model.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    private void ApplyAvx2Optimizations(TModel model)
    {
        // In a real implementation, this would apply AVX2-specific optimizations
        
        // For example, it might:
        // - Use AVX2 intrinsics for faster matrix operations
        // - Optimize memory access patterns for AVX2
        // - Use AVX2-optimized activation functions
        
        // This is a placeholder for the actual implementation
    }
    
    /// <summary>
    /// Determines if GPU acceleration would be beneficial for a model.
    /// </summary>
    /// <param name="model">The model to evaluate.</param>
    /// <returns>True if GPU acceleration would be beneficial, otherwise false.</returns>
    private bool IsGpuBeneficial(TModel model)
    {
        // In a real implementation, this would analyze the model to determine
        // if GPU acceleration would be beneficial
        
        // For example, it might consider:
        // - Model size (too small might not benefit from GPU)
        // - Operation types (some operations work better on CPU)
        // - Memory transfer overhead vs. computation time
        
        // For this example, we'll use a simple heuristic based on parameter count
        // If the model has more than 10 million parameters, GPU might be beneficial
        return model.GetParameters().Length > 10_000_000;
    }
    
    /// <summary>
    /// Applies GPU optimizations to a model.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    private void ApplyGpuOptimizations(TModel model)
    {
        // In a real implementation, this would apply GPU-specific optimizations
        
        // For example, it might:
        // - Set up GPU memory for model parameters
        // - Configure kernel launch parameters
        // - Optimize memory transfers between CPU and GPU
        
        // This is a placeholder for the actual implementation
    }
    
    /// <summary>
    /// Applies NPU optimizations to a model.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    private void ApplyNpuOptimizations(TModel model)
    {
        // In a real implementation, this would apply NPU-specific optimizations
        
        // For example, it might:
        // - Convert the model to a format supported by the NPU
        // - Configure NPU-specific parameters
        // - Set up efficient memory sharing between CPU and NPU
        
        // This is a placeholder for the actual implementation
    }
    
    /// <summary>
    /// Optimizes sparse format for hardware.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    /// <param name="prunedModel">The model as an IPrunedModel.</param>
    private void OptimizeSparseFormat(TModel model, IPrunedModel<double, TInput, TOutput> prunedModel)
    {
        // In a real implementation, this would convert the sparse format
        // to the most efficient format for the hardware
        
        // For example:
        // - CSR format for some CPUs
        // - ELLPACK for some GPUs
        // - Block-sparse format for others
        
        // This is a placeholder for the actual implementation
    }
    
    /// <summary>
    /// Applies sparse matrix optimizations to a model.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    private void ApplySparseOptimizations(TModel model)
    {
        // In a real implementation, this would apply sparse-specific optimizations
        
        // For example, it might:
        // - Configure sparse matrix multiplication routines
        // - Set up efficient sparse data structures
        // - Optimize memory access patterns for sparse operations
        
        // This is a placeholder for the actual implementation
    }
    
    /// <summary>
    /// Applies NVIDIA-specific sparse optimizations to a model.
    /// </summary>
    /// <param name="model">The model to optimize.</param>
    private void ApplyNvidiaSparseOptimizations(TModel model)
    {
        // In a real implementation, this would apply NVIDIA-specific sparse optimizations
        
        // For example, it might:
        // - Use NVIDIA's sparse tensor cores
        // - Configure cuSPARSE parameters
        // - Optimize for specific NVIDIA GPU architectures
        
        // This is a placeholder for the actual implementation
    }

    /// <summary>
    /// Compresses a model according to hardware acceleration requirements.
    /// </summary>
    /// <param name="model">The model to compress.</param>
    /// <returns>The compressed model optimized for hardware.</returns>
    protected override TModel CompressModel(TModel model)
    {
        // In the InferenceOptimizer, "compression" is actually optimization for hardware
        return OptimizeForHardware(model);
    }

    /// <summary>
    /// Measures the accuracy of a model on the provided test data.
    /// </summary>
    /// <param name="model">The model to evaluate.</param>
    /// <param name="testInputs">The test inputs.</param>
    /// <param name="expectedOutputs">The expected outputs for the test inputs.</param>
    /// <returns>A value representing the model's accuracy (higher is better).</returns>
    protected override double MeasureAccuracy(TModel model, TInput[] testInputs, TOutput[] expectedOutputs)
    {
        // This is a simple implementation that could be expanded based on the model type
        double totalCorrect = 0;
        
        for (int i = 0; i < testInputs.Length; i++)
        {
            var predicted = model.Predict(testInputs[i]);
            
            // Accuracy calculation would depend on the model type and output format
            // For this example, we'll use a dummy check just to fulfill the contract
            if (predicted != null && predicted.Equals(expectedOutputs[i]))
            {
                totalCorrect++;
            }
        }
        
        return totalCorrect / testInputs.Length;
    }

    /// <summary>
    /// Runs inference with the model on a single input.
    /// </summary>
    /// <param name="model">The model to use.</param>
    /// <param name="input">The input to process.</param>
    /// <returns>The model's output for the input.</returns>
    protected override TOutput RunInference(TModel model, TInput input)
    {
        return model.Predict(input);
    }

    // SerializeModelToStream is now handled by the base class

    /// <summary>
    /// Deserializes a model from a file.
    /// </summary>
    /// <param name="filePath">The file path from which to load the model.</param>
    /// <returns>The deserialized model.</returns>
    protected override TModel DeserializeModelFromFile(string filePath)
    {
        // Use the base class implementation from ModelCompressorBase
        return DeserializeCompressedModel(filePath);
    }

    /// <summary>
    /// Gets the compression technique used by this compressor.
    /// </summary>
    /// <returns>CompressionTechnique.None for hardware optimization</returns>
    protected override CompressionTechnique GetCompressionTechnique()
    {
        return CompressionTechnique.None;
    }
    
    /// <summary>
    /// Gets the name of the compression technique being used.
    /// </summary>
    /// <returns>A string representing the compression technique.</returns>
    protected override string GetCompressionTechniqueName()
    {
        return "HardwareOptimization";
    }
}

/// <summary>
/// Container for hardware capabilities detected by the InferenceOptimizer.
/// </summary>
/// <remarks>
/// <para>
/// This class stores information about the detected hardware capabilities that
/// can be used for optimization.
/// </para>
/// <para><b>For Beginners:</b> This stores information about what your hardware can do.
/// 
/// It includes details like:
/// - What CPU features are available
/// - Whether a GPU is present and what type
/// - Whether special accelerators are available
/// - What memory architecture is being used
/// 
/// This information helps the optimizer choose the right optimizations.
/// </para>
/// </remarks>
public class HardwareCapabilities
{
    /// <summary>
    /// Gets or sets a value indicating whether SIMD instructions are supported.
    /// </summary>
    /// <remarks>
    /// <para>
    /// SIMD (Single Instruction, Multiple Data) allows performing the same operation
    /// on multiple data elements simultaneously.
    /// </para>
    /// <para><b>For Beginners:</b> SIMD lets the CPU process multiple numbers at once.
    /// 
    /// Instead of adding numbers one at a time, SIMD can add 4, 8, or 16 numbers
    /// in a single operation, which can greatly speed up mathematical operations.
    /// </para>
    /// </remarks>
    public bool HasSIMD { get; set; }
    
    /// <summary>
    /// Gets or sets a value indicating whether AVX2 instructions are supported.
    /// </summary>
    /// <remarks>
    /// <para>
    /// AVX2 (Advanced Vector<double> Extensions 2) is a SIMD instruction set that provides
    /// 256-bit vector operations.
    /// </para>
    /// <para><b>For Beginners:</b> AVX2 is a specific set of instructions for faster math.
    /// 
    /// It allows processing even more numbers at once than basic SIMD,
    /// and is particularly useful for neural network computations.
    /// </para>
    /// </remarks>
    public bool HasAvx2 { get; set; }
    
    /// <summary>
    /// Gets or sets a value indicating whether int8 operations are accelerated.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Many modern CPUs have special instructions for accelerating 8-bit integer operations,
    /// which are commonly used in quantized neural networks.
    /// </para>
    /// <para><b>For Beginners:</b> This indicates if your CPU can process 8-bit numbers quickly.
    /// 
    /// Quantized models use 8-bit integers instead of 32-bit floating point,
    /// and some CPUs have special instructions to process these faster.
    /// </para>
    /// </remarks>
    public bool HasInt8Acceleration { get; set; }
    
    /// <summary>
    /// Gets or sets a value indicating whether sparse matrix operations are accelerated.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Some hardware has special support for accelerating operations on sparse matrices
    /// (matrices with many zero values), which is useful for pruned models.
    /// </para>
    /// <para><b>For Beginners:</b> This indicates if your hardware can handle sparse matrices efficiently.
    /// 
    /// Pruned models have many zero weights, and this special acceleration can
    /// skip computations involving these zeros, making inference faster.
    /// </para>
    /// </remarks>
    public bool HasSparseAcceleration { get; set; }
    
    /// <summary>
    /// Gets or sets a value indicating whether a GPU is available.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Graphics Processing Units (GPUs) can significantly accelerate neural network inference.
    /// </para>
    /// <para><b>For Beginners:</b> This indicates if a graphics card is available for acceleration.
    /// 
    /// GPUs have many small processing cores that can perform many calculations in parallel,
    /// which is ideal for neural network operations.
    /// </para>
    /// </remarks>
    public bool HasGPU { get; set; }
    
    /// <summary>
    /// Gets or sets the type of GPU available.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Different GPU types (NVIDIA, AMD, Intel, etc.) have different capabilities and
    /// require different optimization strategies.
    /// </para>
    /// <para><b>For Beginners:</b> This tells you what brand or type of GPU is available.
    /// 
    /// Different GPU brands have different capabilities and programming interfaces,
    /// so the optimizer needs to know which type it's working with.
    /// </para>
    /// </remarks>
    public string GPUType { get; set; } = "None";
    
    /// <summary>
    /// Gets or sets a value indicating whether a Neural Processing Unit is available.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Neural Processing Units (NPUs) are specialized hardware accelerators designed
    /// specifically for neural network operations.
    /// </para>
    /// <para><b>For Beginners:</b> This indicates if a special AI chip is available.
    /// 
    /// NPUs are dedicated chips designed specifically for AI workloads, like Apple's Neural Engine
    /// or Google's Edge TPU, and can be much more efficient than CPUs or GPUs for certain models.
    /// </para>
    /// </remarks>
    public bool HasNPU { get; set; }
    
    /// <summary>
    /// Gets or sets the type of NPU available.
    /// </summary>
    /// <remarks>
    /// <para>
    /// Different NPU types have different capabilities and require different optimization approaches.
    /// </para>
    /// <para><b>For Beginners:</b> This tells you what kind of AI chip is available.
    /// 
    /// Different NPUs support different operations and have different programming interfaces,
    /// so the optimizer needs to know which type it's working with.
    /// </para>
    /// </remarks>
    public string NPUType { get; set; } = "None";
}

/// <summary>
/// Interface for models that support hardware-specific optimization.
/// </summary>
/// <remarks>
/// <para>
/// This interface should be implemented by models that can apply their own
/// hardware-specific optimizations.
/// </para>
/// <para><b>For Beginners:</b> This marks a model as able to optimize itself for specific hardware.
/// 
/// Models that implement this interface know how to:
/// - Analyze what hardware features are available
/// - Apply optimizations specific to those features
/// - Reconfigure themselves for optimal performance
/// </para>
/// </remarks>
/// <typeparam name="TModel">The type of the model.</typeparam>
/// <typeparam name="TInput">The input type for the model.</typeparam>
/// <typeparam name="TOutput">The output type for the model.</typeparam>
public interface IHardwareOptimizable<TModel, TInput, TOutput>
    where TModel : class, IFullModel<double, TInput, TOutput>
{
    /// <summary>
    /// Optimizes the model for the specified hardware capabilities.
    /// </summary>
    /// <param name="capabilities">The hardware capabilities to optimize for.</param>
    /// <returns>The hardware-optimized model.</returns>
    /// <remarks>
    /// <para>
    /// This method applies optimizations to the model based on the specified hardware capabilities.
    /// </para>
    /// <para><b>For Beginners:</b> This lets a model optimize itself for specific hardware.
    /// 
    /// Given information about the hardware:
    /// - The model can apply specific optimizations
    /// - It can reconfigure itself for better performance
    /// - It can choose the best algorithms and data structures
    /// 
    /// This allows for more specialized optimizations than the general InferenceOptimizer provides.
    /// </para>
    /// </remarks>
    TModel OptimizeForHardware(HardwareCapabilities capabilities);
}