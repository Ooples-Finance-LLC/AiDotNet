{
  "bug_id": "BUG-003",
  "title": "Fix Build Errors in NeuralArchitectureSearch.cs",
  "already_fixed": false,
  "file_path": "C:\\Users\\cheat\\source\\repos\\AiDotNet\\src\\AutoML\\NeuralArchitectureSearch.cs",
  "issues_found": [
    {
      "type": "Unclosed Comment Block",
      "location": "Line 382",
      "description": "Multi-line comment starting at line 382 is not closed, causing CS1035 error",
      "error_code": "CS1035"
    },
    {
      "type": "Missing Closing Brace",
      "location": "Line 380",
      "description": "Missing closing brace for commented-out code block, causing CS1513 error",
      "error_code": "CS1513"
    },
    {
      "type": "Duplicate Using Statement",
      "location": "Lines 9 and 12",
      "description": "using AiDotNet.Models; appears twice",
      "error_code": "Warning"
    },
    {
      "type": "Duplicate Method",
      "location": "Lines 94-130 and 160-214",
      "description": "SearchAsync method is defined twice with same signature",
      "error_code": "CS0111 (will appear once comment fixed)"
    },
    {
      "type": "Duplicate Method",
      "location": "Lines 219-279 and 801-817",
      "description": "SuggestNextTrialAsync method is defined twice with same signature",
      "error_code": "CS0111 (will appear once comment fixed)"
    },
    {
      "type": "Type Mismatch",
      "location": "Lines 918-919",
      "description": "NeuralArchitectureSearchModel<T> uses INeuralNetworkModel<double> and non-generic ArchitectureCandidate instead of T and ArchitectureCandidate<T>",
      "error_code": "CS0029 (type conversion issues)"
    }
  ],
  "fixes": [
    {
      "fix_number": 1,
      "description": "Close the multi-line comment block",
      "location": "Line 424",
      "old_code": "            UpdateTopArchitectures(new List<ArchitectureCandidate<T>> { candidate });",
      "new_code": "            UpdateTopArchitectures(new List<ArchitectureCandidate<T>> { candidate });\n            */",
      "explanation": "The comment block starting at line 382 needs to be properly closed"
    },
    {
      "fix_number": 2,
      "description": "Add missing closing brace",
      "location": "After line 424 (new line 425)",
      "old_code": "            */",
      "new_code": "            */\n        }",
      "explanation": "The RunGradientBasedSearch method needs a closing brace"
    },
    {
      "fix_number": 3,
      "description": "Remove duplicate using statement",
      "location": "Line 12",
      "old_code": "using AiDotNet.Models;",
      "new_code": "",
      "explanation": "Remove the duplicate using statement at line 12"
    },
    {
      "fix_number": 4,
      "description": "Remove first duplicate SearchAsync method",
      "location": "Lines 94-130",
      "old_code": "        public override async Task<IFullModel<T, Tensor<T>, Tensor<T>>> SearchAsync(\n            Tensor<T> inputs,\n            Tensor<T> targets,\n            Tensor<T> validationInputs,\n            Tensor<T> validationTargets,\n            TimeSpan timeLimit,\n            CancellationToken cancellationToken = default)\n        {\n            Status = AutoMLStatus.Running;\n            var startTime = DateTime.UtcNow;\n            \n            try\n            {\n                await SearchAsync(inputs, targets, validationInputs, validationTargets);\n                \n                if (topArchitectures.Any())\n                {\n                    var bestCandidate = topArchitectures.First();\n                    BestModel = BuildNetworkFromCandidate(bestCandidate);\n                    var ops = MathHelper.GetNumericOperations<T>();\n                    BestScore = Convert.ToDouble(bestCandidate.Fitness);\n                }\n                \n                Status = AutoMLStatus.Completed;\n                return BestModel ?? throw new InvalidOperationException(\"No valid model found\");\n            }\n            catch (OperationCanceledException)\n            {\n                Status = AutoMLStatus.Cancelled;\n                throw;\n            }\n            catch (Exception)\n            {\n                Status = AutoMLStatus.Failed;\n                throw;\n            }\n        }",
      "new_code": "",
      "explanation": "Remove the first SearchAsync method (lines 94-130) as it's less complete than the second one (lines 160-214)"
    },
    {
      "fix_number": 5,
      "description": "Remove first duplicate SuggestNextTrialAsync method",
      "location": "Lines 219-279",
      "old_code": "        public override async Task<Dictionary<string, object>> SuggestNextTrialAsync()\n        {\n            return await Task.Run(() =>\n            {\n                var parameters = new Dictionary<string, object>();\n\n                // Select search strategy\n                parameters[\"strategy\"] = strategy.ToString();\n\n                // Sample architecture hyperparameters based on search space\n                var numLayers = random.Next(2, maxLayers + 1);\n                parameters[\"num_layers\"] = numLayers;\n\n                var layerConfigs = new List<Dictionary<string, object>>();\n                for (int i = 0; i < numLayers; i++)\n                {\n                    var layerType = availableLayerTypes[random.Next(availableLayerTypes.Count)];\n                    var layerConfig = new Dictionary<string, object>\n                    {\n                        [\"type\"] = layerType.ToString(),\n                        [\"units\"] = random.Next(16, maxNeuronsPerLayer + 1),\n                        [\"activation\"] = availableActivations[random.Next(availableActivations.Count)].ToString()\n                    };\n\n                    // Add layer-specific parameters\n                    switch (layerType)\n                    {\n                        case LayerType.Convolutional:\n                            layerConfig[\"filters\"] = random.Next(8, 512);\n                            layerConfig[\"kernel_size\"] = searchSpace.KernelSizes[random.Next(searchSpace.KernelSizes.Length)];\n                            layerConfig[\"stride\"] = 1;\n                            break;\n\n                        case LayerType.Dropout:\n                            layerConfig[\"dropout_rate\"] = searchSpace.DropoutRates[random.Next(searchSpace.DropoutRates.Length)];\n                            break;\n\n                        case LayerType.MaxPooling:\n                        case LayerType.AveragePooling:\n                            layerConfig[\"pool_size\"] = 2;\n                            break;\n\n                        case LayerType.LSTM:\n                        case LayerType.GRU:\n                            layerConfig[\"return_sequences\"] = i < numLayers - 1;\n                            break;\n                    }\n\n                    layerConfigs.Add(layerConfig);\n                }\n\n                parameters[\"layers\"] = layerConfigs;\n\n                // Add optimization parameters\n                parameters[\"learning_rate\"] = 0.001 * Math.Pow(10, random.NextDouble() * 2 - 1);\n                parameters[\"batch_size\"] = new[] { 16, 32, 64, 128 }[random.Next(4)];\n                parameters[\"epochs\"] = random.Next(5, 20);\n\n                return parameters;\n            });\n        }",
      "new_code": "",
      "explanation": "Remove the first SuggestNextTrialAsync method (lines 219-279) as the second one (lines 801-817) is simpler and sufficient"
    },
    {
      "fix_number": 6,
      "description": "Fix type mismatch in NeuralArchitectureSearchModel constructor and fields",
      "location": "Lines 918-921",
      "old_code": "        private readonly INeuralNetworkModel<double> _innerModel = default!;\n        private readonly ArchitectureCandidate _architecture = default!;\n\n        public NeuralArchitectureSearchModel(INeuralNetworkModel<double> innerModel, ArchitectureCandidate architecture)",
      "new_code": "        private readonly INeuralNetworkModel<T> _innerModel = default!;\n        private readonly ArchitectureCandidate<T> _architecture = default!;\n\n        public NeuralArchitectureSearchModel(INeuralNetworkModel<T> innerModel, ArchitectureCandidate<T> architecture)",
      "explanation": "Change INeuralNetworkModel<double> to INeuralNetworkModel<T> and ArchitectureCandidate to ArchitectureCandidate<T> to match the generic type T"
    }
  ],
  "complete_fixed_code": "using System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing AiDotNet.Enums;\nusing AiDotNet.Interfaces;\nusing AiDotNet.LinearAlgebra;\nusing AiDotNet.Models;\nusing AiDotNet.NeuralNetworks;\nusing AiDotNet.Optimizers;\nusing AiDotNet.Helpers;\nusing AiDotNet.NeuralNetworks.Layers;\n\nnamespace AiDotNet.AutoML\n{\n    /// <summary>\n    /// Neural Architecture<T> Search (NAS) for automatically designing neural network architectures\n    /// </summary>\n    /// <typeparam name=\"T\">The numeric type used for calculations</typeparam>\n    public class NeuralArchitectureSearch<T> : AutoMLModelBase<T, Tensor<T>, Tensor<T>>\n        where T : struct, IComparable<T>, IConvertible, IEquatable<T>\n    {\n        private readonly NeuralArchitectureSearchStrategy strategy;\n        private readonly int maxLayers;\n        private readonly int maxNeuronsPerLayer;\n        private readonly List<LayerType> availableLayerTypes;\n        private readonly List<ActivationFunction> availableActivations;\n        private readonly T resourceBudget;\n        private readonly int populationSize;\n        private readonly int generations;\n        \n        // Search space definition\n        private readonly SearchSpace<T> searchSpace;\n        \n        // Best architectures found\n        private readonly List<ArchitectureCandidate<T>> topArchitectures;\n        \n        // Random number generator\n        private readonly Random random = new Random();\n        \n        public NeuralArchitectureSearch(\n            NeuralArchitectureSearchStrategy strategy = NeuralArchitectureSearchStrategy.Evolutionary,\n            int maxLayers = 10,\n            int maxNeuronsPerLayer = 512,\n            T? resourceBudget = null,\n            int populationSize = 50,\n            int generations = 20,\n            string modelName = \"NeuralArchitectureSearch\")\n        {\n            this.strategy = strategy;\n            this.maxLayers = maxLayers;\n            this.maxNeuronsPerLayer = maxNeuronsPerLayer;\n            var ops = MathHelper.GetNumericOperations<T>();\n            this.resourceBudget = resourceBudget ?? ops.FromDouble(100.0);\n            this.populationSize = populationSize;\n            this.generations = generations;\n            this.random = new Random();\n\n            // Define available layer types and activations\n            availableLayerTypes = new List<LayerType>\n            {\n                LayerType.FullyConnected,\n                LayerType.Convolutional,\n                LayerType.LSTM,\n                LayerType.GRU,\n                LayerType.Dropout,\n                LayerType.BatchNormalization,\n                LayerType.MaxPooling,\n                LayerType.AveragePooling\n            };\n\n            availableActivations = new List<ActivationFunction>\n            {\n                ActivationFunction.ReLU,\n                ActivationFunction.LeakyReLU,\n                ActivationFunction.ELU,\n                ActivationFunction.Tanh,\n                ActivationFunction.Sigmoid,\n                ActivationFunction.Swish,\n                ActivationFunction.GELU\n            };\n            \n            searchSpace = new SearchSpace<T>();\n            topArchitectures = new List<ArchitectureCandidate<T>>();\n            \n            InitializeSearchSpace();\n        }\n        \n        /// <summary>\n        /// Run the neural architecture search\n        /// </summary>\n        private async Task SearchAsync(Tensor<T> trainData, Tensor<T> trainLabels, Tensor<T> valData, Tensor<T> valLabels)\n        {\n            switch (strategy)\n            {\n                case NeuralArchitectureSearchStrategy.Evolutionary:\n                    RunEvolutionarySearch(trainData, trainLabels, valData, valLabels);\n                    break;\n                case NeuralArchitectureSearchStrategy.ReinforcementLearning:\n                    RunReinforcementLearningSearch(trainData, trainLabels, valData, valLabels);\n                    break;\n                case NeuralArchitectureSearchStrategy.GradientBased:\n                    RunGradientBasedSearch(trainData, trainLabels, valData, valLabels);\n                    break;\n                case NeuralArchitectureSearchStrategy.RandomSearch:\n                    RunRandomSearch(trainData, trainLabels, valData, valLabels);\n                    break;\n                case NeuralArchitectureSearchStrategy.BayesianOptimization:\n                    await RunBayesianOptimizationSearchAsync(trainData, trainLabels, valData, valLabels);\n                    break;\n            }\n        }\n\n        /// <summary>\n        /// Searches for the best neural architecture configuration\n        /// </summary>\n        public override async Task<IFullModel<T, Tensor<T>, Tensor<T>>> SearchAsync(\n            Tensor<T> inputs,\n            Tensor<T> targets,\n            Tensor<T> validationInputs,\n            Tensor<T> validationTargets,\n            TimeSpan timeLimit,\n            CancellationToken cancellationToken = default)\n        {\n            Status = AutoMLStatus.Running;\n            var startTime = DateTime.UtcNow;\n\n            try\n            {\n                Console.WriteLine($\"Neural Architecture Search: Strategy={strategy}, Time limit={timeLimit}\");\n\n                // Run the architecture search\n                await Task.Run(() =>\n                {\n                    Search(inputs, targets, validationInputs, validationTargets);\n                }, cancellationToken);\n\n                // Get the best architecture found\n                var bestArchitecture = GetBestArchitecture();\n\n                if (bestArchitecture == null)\n                {\n                    Status = AutoMLStatus.Failed;\n                    throw new InvalidOperationException(\"No valid architecture found during search\");\n                }\n\n                // Build the final model from the best architecture\n                var finalModel = BuildNetworkFromCandidate(bestArchitecture);\n\n                // Convert to IFullModel (wrap the neural network)\n                var wrappedModel = new NeuralArchitectureSearchModel<T>(finalModel, bestArchitecture);\n\n                BestModel = wrappedModel;\n                BestScore = bestArchitecture.Fitness;\n                Status = AutoMLStatus.Completed;\n\n                Console.WriteLine($\"Neural Architecture Search completed. Best fitness: {BestScore:F4}, Architecture layers: {bestArchitecture.Layers.Count}\");\n\n                return BestModel;\n            }\n            catch (OperationCanceledException)\n            {\n                Status = AutoMLStatus.Cancelled;\n                throw;\n            }\n            catch (Exception ex)\n            {\n                Status = AutoMLStatus.Failed;\n                throw new InvalidOperationException($\"Neural architecture search failed: {ex.Message}\", ex);\n            }\n        }\n        \n        /// <summary>\n        /// Evolutionary search strategy\n        /// </summary>\n        private void RunEvolutionarySearch(Tensor<T> trainData, Tensor<T> trainLabels, Tensor<T> valData, Tensor<T> valLabels)\n        {\n            // Initialize population\n            var population = InitializePopulation(populationSize);\n            \n            for (int gen = 0; gen < generations; gen++)\n            {\n                // Evaluate fitness of each architecture\n                foreach (var candidate in population)\n                {\n                    if (!candidate.IsEvaluated)\n                    {\n                        EvaluateArchitecture(candidate, trainData, trainLabels, valData, valLabels);\n                    }\n                }\n                \n                // Sort by fitness\n                population = population.OrderByDescending(c => c.Fitness).ToList();\n                \n                // Store top architectures\n                UpdateTopArchitectures(population.Take(5).ToList());\n                \n                // Selection\n                var parents = TournamentSelection(population, populationSize / 2);\n                \n                // Crossover and mutation\n                var offspring = new List<ArchitectureCandidate<T>>();\n                while (offspring.Count < populationSize)\n                {\n                    var parent1 = parents[random.Next(parents.Count)];\n                    var parent2 = parents[random.Next(parents.Count)];\n                    \n                    var child = Crossover(parent1, parent2);\n                    child = Mutate(child);\n                    \n                    offspring.Add(child);\n                }\n                \n                // Replace population\n                population = offspring;\n                \n                LogProgress($\"Generation {gen + 1}/{generations}, Best fitness: {topArchitectures.First().Fitness:F4}\");\n            }\n        }\n        \n        /// <summary>\n        /// Reinforcement learning-based search\n        /// </summary>\n        private void RunReinforcementLearningSearch(Tensor<T> trainData, Tensor<T> trainLabels, Tensor<T> valData, Tensor<T> valLabels)\n        {\n            // Use a controller network to generate architectures\n            var controller = new ControllerNetwork<T>(searchSpace);\n            var rewardHistory = new List<double>();\n            \n            var ops = MathHelper.GetNumericOperations<T>();\n            var resourceBudgetInt = Convert.ToInt32(resourceBudget);\n            for (int episode = 0; episode < resourceBudgetInt; episode++)\n            {\n                // Generate architecture using controller\n                var architecture = controller.GenerateArchitecture();\n                var candidate = CreateCandidateFromArchitecture(architecture);\n                \n                // Evaluate architecture\n                EvaluateArchitecture(candidate, trainData, trainLabels, valData, valLabels);\n                \n                // Use validation accuracy as reward\n                var reward = candidate.Fitness;\n                rewardHistory.Add(Convert.ToDouble(reward));\n                \n                // Update controller using REINFORCE algorithm\n                controller.UpdateWithReward(reward);\n                \n                // Update top architectures\n                UpdateTopArchitectures(new List<ArchitectureCandidate<T>> { candidate });\n                \n                // Calculate average reward for the last 10 episodes\n                var recentRewards = rewardHistory.Count > 10 \n                    ? rewardHistory.Skip(rewardHistory.Count - 10).Take(10) \n                    : rewardHistory;\n                var avgReward = recentRewards.Average();\n                \n                LogProgress($\"Episode {episode + 1}, Reward: {reward:F4}, Avg reward: {avgReward:F4}\");\n            }\n        }\n        \n        /// <summary>\n        /// Gradient-based search (DARTS-style)\n        /// </summary>\n        private void RunGradientBasedSearch(Tensor<T> trainData, Tensor<T> trainLabels, Tensor<T> valData, Tensor<T> valLabels)\n        {\n            // TODO: Gradient-based NAS requires SuperNet to implement IFullModel<T, Tensor<T>, Tensor<T>>\n            // TODO: The optimizer API has changed - Step() no longer accepts parameters/gradients\n            // TODO: Need to refactor SuperNet to work with the new optimizer interface\n\n            // For now, fall back to random search\n            LogProgress(\"Gradient-based search not yet implemented with new optimizer API. Using random search instead.\");\n            RunRandomSearch(trainData, trainLabels, valData, valLabels);\n\n            /* Original implementation - needs refactoring:\n            // Create a supernet with learnable architecture parameters\n            var supernet = new SuperNet<T>(searchSpace);\n            \n            // Initialize learning rates for architecture and weight parameters\n            var ops = MathHelper.GetNumericOperations<T>();\n            var architectureLearningRate = ops.FromDouble(0.001);\n            var weightsLearningRate = ops.FromDouble(0.001);\n            var momentum = ops.FromDouble(0.9);\n            \n            // Initialize momentum buffers\n            var archMomentum = new List<Tensor<T>>();\n            var weightMomentum = new List<Tensor<T>>();\n            \n            for (int epoch = 0; epoch < 50; epoch++)\n            {\n                // Update architecture parameters on validation set\n                var valLoss = supernet.ComputeValidationLoss(valData, valLabels);\n                supernet.BackwardArchitecture(valLoss);\n                var archParams = supernet.GetArchitectureParameters();\n                var archGrads = supernet.GetArchitectureGradients();\n                \n                // Apply gradient descent with momentum for architecture parameters\n                UpdateParametersWithMomentum(archParams, archGrads, archMomentum, architectureLearningRate, momentum);\n                \n                // Update network weights on training set\n                var trainLoss = supernet.ComputeTrainingLoss(trainData, trainLabels);\n                supernet.BackwardWeights(trainLoss);\n                var weightParams = supernet.GetWeightParameters();\n                var weightGrads = supernet.GetWeightGradients();\n                \n                // Apply gradient descent with momentum for weight parameters\n                UpdateParametersWithMomentum(weightParams, weightGrads, weightMomentum, weightsLearningRate, momentum);\n                \n                LogProgress($\"Epoch {epoch + 1}, Train loss: {trainLoss:F4}, Val loss: {valLoss:F4}\");\n            }\n\n            // Derive final architecture from supernet\n            var finalArchitecture = supernet.DeriveArchitecture();\n            var candidate = CreateCandidateFromArchitecture(finalArchitecture);\n            EvaluateArchitecture(candidate, trainData, trainLabels, valData, valLabels);\n            UpdateTopArchitectures(new List<ArchitectureCandidate<T>> { candidate });\n            */\n        }\n        \n        /// <summary>\n        /// Update parameters using gradient descent with momentum\n        /// </summary>\n        private void UpdateParametersWithMomentum(List<Tensor<T>> parameters, List<Tensor<T>> gradients, \n            List<Tensor<T>> momentumBuffers, T learningRate, T momentum)\n        {\n            var ops = MathHelper.GetNumericOperations<T>();\n            \n            // Initialize momentum buffers if needed\n            while (momentumBuffers.Count < parameters.Count)\n            {\n                var param = parameters[momentumBuffers.Count];\n                momentumBuffers.Add(new Tensor<T>(param.Shape));\n            }\n            \n            // Update each parameter\n            for (int i = 0; i < parameters.Count; i++)\n            {\n                var param = parameters[i];\n                var grad = gradients[i];\n                var momentumBuffer = momentumBuffers[i];\n                \n                // Update momentum: m = momentum * m + (1 - momentum) * grad\n                for (int j = 0; j < param.Length; j++)\n                {\n                    var currentMomentum = ops.Multiply(momentum, momentumBuffer[j]);\n                    var gradContribution = ops.Multiply(ops.Subtract(ops.One, momentum), grad[j]);\n                    momentumBuffer[j] = ops.Add(currentMomentum, gradContribution);\n                    \n                    // Update parameter: param = param - learning_rate * m\n                    param[j] = ops.Subtract(param[j], ops.Multiply(learningRate, momentumBuffer[j]));\n                }\n            }\n        }\n        \n        /// <summary>\n        /// Random search baseline\n        /// </summary>\n        private void RunRandomSearch(Tensor<T> trainData, Tensor<T> trainLabels, Tensor<T> valData, Tensor<T> valLabels)\n        {\n            var evaluations = Convert.ToInt32(resourceBudget);\n            \n            for (int i = 0; i < evaluations; i++)\n            {\n                // Generate random architecture\n                var candidate = GenerateRandomArchitecture();\n                \n                // Evaluate\n                EvaluateArchitecture(candidate, trainData, trainLabels, valData, valLabels);\n                \n                // Update top architectures\n                UpdateTopArchitectures(new List<ArchitectureCandidate<T>> { candidate });\n                \n                var bestFitness = topArchitectures.FirstOrDefault()?.Fitness;\n                var bestFitnessValue = bestFitness != null ? Convert.ToDouble(bestFitness) : 0.0;\n                LogProgress($\"Evaluation {i + 1}/{evaluations}, Best fitness: {bestFitnessValue:F4}\");\n            }\n        }\n        \n        /// <summary>\n        /// Bayesian optimization search\n        /// </summary>\n        private async Task RunBayesianOptimizationSearchAsync(Tensor<T> trainData, Tensor<T> trainLabels, Tensor<T> valData, Tensor<T> valLabels)\n        {\n            var bayesianOptimizer = new BayesianOptimizationAutoML<T, Tensor<T>, Tensor<T>>(numInitialPoints: 10, explorationWeight: 2.0);\n            \n            // Define hyperparameter space for architectures\n            var searchSpace = new Dictionary<string, ParameterRange>();\n            searchSpace[\"num_layers\"] = new ParameterRange\n            {\n                Type = ParameterType.Integer,\n                MinValue = 2,\n                MaxValue = maxLayers\n            };\n            searchSpace[\"neurons_per_layer\"] = new ParameterRange\n            {\n                Type = ParameterType.Integer,\n                MinValue = 16,\n                MaxValue = maxNeuronsPerLayer\n            };\n            searchSpace[\"dropout_rate\"] = new ParameterRange\n            {\n                Type = ParameterType.Continuous,\n                MinValue = 0.0,\n                MaxValue = 0.5\n            };\n            searchSpace[\"learning_rate\"] = new ParameterRange\n            {\n                Type = ParameterType.Continuous,\n                MinValue = 0.0001,\n                MaxValue = 0.01,\n                LogScale = true\n            };\n            \n            bayesianOptimizer.SetSearchSpace(searchSpace);\n            bayesianOptimizer.SetOptimizationMetric(MetricType.Accuracy, maximize: true);\n            \n            // Run Bayesian optimization\n            try\n            {\n                var bestModel = await bayesianOptimizer.SearchAsync(\n                    trainData, \n                    trainLabels, \n                    valData, \n                    valLabels, \n                    TimeSpan.FromHours(1)\n                );\n                \n                // Convert results to architecture candidates\n                var trialHistory = bayesianOptimizer.GetTrialHistory();\n                foreach (var trial in trialHistory.Where(t => t.IsSuccessful).OrderByDescending(t => t.Score).Take(10))\n                {\n                    var candidate = CreateCandidateFromHyperparameters(trial.Parameters);\n                    var ops = MathHelper.GetNumericOperations<T>();\n                    candidate.Fitness = ops.FromDouble(trial.Score);\n                    UpdateTopArchitectures(new List<ArchitectureCandidate<T>> { candidate });\n                }\n            }\n            catch (Exception ex)\n            {\n                LogError($\"Bayesian optimization failed: {ex.Message}\");\n            }\n        }\n        \n        /// <summary>\n        /// Evaluate a candidate architecture\n        /// </summary>\n        private void EvaluateArchitecture(ArchitectureCandidate<T> candidate, Tensor<T> trainData, Tensor<T> trainLabels, Tensor<T> valData, Tensor<T> valLabels)\n        {\n            try\n            {\n                // Build the network\n                var network = BuildNetworkFromCandidate(candidate);\n\n                // Train for limited epochs (early stopping)\n                // Note: In production, the optimizer would be created with the actual model\n                // For now, we'll use the network's training method directly\n                var maxEpochs = 10; // Quick evaluation\n\n                for (int epoch = 0; epoch < maxEpochs; epoch++)\n                {\n                    // Train one epoch\n                    network.Train(trainData, trainLabels);\n                }\n                \n                // Evaluate on validation set\n                var valAccuracy = EvaluateAccuracy(network, valData, valLabels);\n                \n                // Compute efficiency metrics\n                var parameters = CountParameters(network);\n                var flops = EstimateFLOPs(network, trainData.Shape);\n                \n                // Combine metrics into fitness score\n                candidate.Fitness = ComputeFitnessScore(valAccuracy, parameters, flops);\n                candidate.ValidationAccuracy = valAccuracy;\n                candidate.Parameters = parameters;\n                candidate.FLOPs = flops;\n                candidate.IsEvaluated = true;\n            }\n            catch (Exception ex)\n            {\n                // Invalid architecture\n                var ops = MathHelper.GetNumericOperations<T>();\n                candidate.Fitness = ops.Zero;\n                candidate.IsEvaluated = true;\n                LogError($\"Failed to evaluate architecture: {ex.Message}\");\n            }\n        }\n        \n        /// <summary>\n        /// Build a neural network from architecture candidate\n        /// </summary>\n        private INeuralNetworkModel<T> BuildNetworkFromCandidate(ArchitectureCandidate<T> candidate)\n        {\n            // For now, create a simple feedforward network\n            // In production, this would create the actual architecture from the candidate\n            var ops = MathHelper.GetNumericOperations<T>();\n            var learningRate = ops.FromDouble(0.001);\n            \n            // Create architecture\n            var architecture = new NeuralNetworkArchitecture<T>(NetworkComplexity.Medium);\n            \n            // Create and return the network\n            var network = new FeedForwardNeuralNetwork<T>(\n                architecture,\n                null, // optimizer will be set during training\n                null, // loss function will be set during training\n                Convert.ToDouble(learningRate)\n            );\n            \n            return network;\n        }\n        \n        /// <summary>\n        /// Initialize search space\n        /// </summary>\n        private void InitializeSearchSpace()\n        {\n            var ops = MathHelper.GetNumericOperations<T>();\n            searchSpace.LayerTypes = availableLayerTypes;\n            searchSpace.ActivationFunctions = availableActivations;\n            searchSpace.MaxLayers = maxLayers;\n            searchSpace.MaxUnitsPerLayer = maxNeuronsPerLayer;\n            searchSpace.MaxFilters = 512;\n            searchSpace.KernelSizes = new Vector<int>(new[] { 1, 3, 5, 7 });\n            searchSpace.DropoutRates = new Vector<T>(new[] { \n                ops.FromDouble(0.0), \n                ops.FromDouble(0.1), \n                ops.FromDouble(0.2), \n                ops.FromDouble(0.3), \n                ops.FromDouble(0.4), \n                ops.FromDouble(0.5) \n            });\n        }\n        \n        /// <summary>\n        /// Initialize population for evolutionary search\n        /// </summary>\n        private List<ArchitectureCandidate<T>> InitializePopulation(int size)\n        {\n            var population = new List<ArchitectureCandidate<T>>();\n            \n            for (int i = 0; i < size; i++)\n            {\n                population.Add(GenerateRandomArchitecture());\n            }\n            \n            return population;\n        }\n        \n        /// <summary>\n        /// Generate a random architecture\n        /// </summary>\n        private ArchitectureCandidate<T> GenerateRandomArchitecture()\n        {\n            var candidate = new ArchitectureCandidate<T>();\n            var numLayers = random.Next(2, maxLayers + 1);\n            \n            for (int i = 0; i < numLayers; i++)\n            {\n                var layerType = availableLayerTypes[random.Next(availableLayerTypes.Count)];\n                var layer = new LayerConfiguration<T>\n                {\n                    Type = layerType,\n                    Units = random.Next(16, maxNeuronsPerLayer + 1),\n                    Activation = availableActivations[random.Next(availableActivations.Count)],\n                    Filters = random.Next(8, 512),\n                    KernelSize = searchSpace.KernelSizes[random.Next(searchSpace.KernelSizes.Length)],\n                    Stride = 1,\n                    PoolSize = 2,\n                    DropoutRate = searchSpace.DropoutRates[random.Next(searchSpace.DropoutRates.Length)],\n                    ReturnSequences = i < numLayers - 1\n                };\n                \n                candidate.Layers.Add(layer);\n            }\n            \n            return candidate;\n        }\n        \n        /// <summary>\n        /// Tournament selection\n        /// </summary>\n        private List<ArchitectureCandidate<T>> TournamentSelection(List<ArchitectureCandidate<T>> population, int numSelected)\n        {\n            var selected = new List<ArchitectureCandidate<T>>();\n            var tournamentSize = 3;\n            \n            while (selected.Count < numSelected)\n            {\n                var tournament = new List<ArchitectureCandidate<T>>();\n                \n                for (int i = 0; i < tournamentSize; i++)\n                {\n                    tournament.Add(population[random.Next(population.Count)]);\n                }\n                \n                selected.Add(tournament.OrderByDescending(c => c.Fitness).First());\n            }\n            \n            return selected;\n        }\n        \n        /// <summary>\n        /// Crossover two parent architectures\n        /// </summary>\n        private ArchitectureCandidate<T> Crossover(ArchitectureCandidate<T> parent1, ArchitectureCandidate<T> parent2)\n        {\n            var child = new ArchitectureCandidate<T>();\n            \n            // Choose crossover point\n            var minLayers = Math.Min(parent1.Layers.Count, parent2.Layers.Count);\n            var crossoverPoint = random.Next(1, minLayers);\n            \n            // Take layers from parent1 up to crossover point\n            for (int i = 0; i < crossoverPoint; i++)\n            {\n                child.Layers.Add(parent1.Layers[i].Clone());\n            }\n            \n            // Take remaining layers from parent2\n            for (int i = crossoverPoint; i < parent2.Layers.Count; i++)\n            {\n                child.Layers.Add(parent2.Layers[i].Clone());\n            }\n            \n            return child;\n        }\n        \n        /// <summary>\n        /// Mutate an architecture\n        /// </summary>\n        private ArchitectureCandidate<T> Mutate(ArchitectureCandidate<T> candidate)\n        {\n            var mutated = candidate.Clone();\n            var mutationRate = 0.1;\n            \n            // Mutate layers\n            foreach (var layer in mutated.Layers)\n            {\n                if (random.NextDouble() < mutationRate)\n                {\n                    // Change layer type\n                    layer.Type = availableLayerTypes[random.Next(availableLayerTypes.Count)];\n                }\n                \n                if (random.NextDouble() < mutationRate)\n                {\n                    // Change units/filters\n                    layer.Units = random.Next(16, maxNeuronsPerLayer + 1);\n                    layer.Filters = random.Next(8, 512);\n                }\n                \n                if (random.NextDouble() < mutationRate)\n                {\n                    // Change activation\n                    layer.Activation = availableActivations[random.Next(availableActivations.Count)];\n                }\n            }\n            \n            // Add/remove layers\n            if (random.NextDouble() < mutationRate && mutated.Layers.Count < maxLayers)\n            {\n                // Add a random layer\n                var newLayer = GenerateRandomArchitecture().Layers.First();\n                mutated.Layers.Insert(random.Next(mutated.Layers.Count), newLayer);\n            }\n            \n            if (random.NextDouble() < mutationRate && mutated.Layers.Count > 2)\n            {\n                // Remove a random layer\n                mutated.Layers.RemoveAt(random.Next(mutated.Layers.Count));\n            }\n            \n            return mutated;\n        }\n        \n        /// <summary>\n        /// Update top architectures list\n        /// </summary>\n        private void UpdateTopArchitectures(List<ArchitectureCandidate<T>> candidates)\n        {\n            topArchitectures.AddRange(candidates.Where(c => c.IsEvaluated));\n            topArchitectures.Sort((a, b) => b.Fitness.CompareTo(a.Fitness));\n            \n            // Keep only top 10\n            if (topArchitectures.Count > 10)\n            {\n                topArchitectures.RemoveRange(10, topArchitectures.Count - 10);\n            }\n        }\n        \n        /// <summary>\n        /// Suggests the next hyperparameters to try\n        /// </summary>\n        public override async Task<Dictionary<string, object>> SuggestNextTrialAsync()\n        {\n            return await Task.Run(() =>\n            {\n                var parameters = new Dictionary<string, object>();\n                \n                // Generate random architecture parameters\n                parameters[\"num_layers\"] = random.Next(2, maxLayers + 1);\n                parameters[\"layer_types\"] = availableLayerTypes[random.Next(availableLayerTypes.Count)];\n                parameters[\"activation\"] = availableActivations[random.Next(availableActivations.Count)];\n                var ops = MathHelper.GetNumericOperations<T>();\n                parameters[\"dropout_rate\"] = ops.FromDouble(random.NextDouble() * 0.5);\n                parameters[\"neurons_per_layer\"] = random.Next(16, maxNeuronsPerLayer + 1);\n                \n                return parameters;\n            });\n        }\n\n        /// <summary>\n        /// Get the best architecture found\n        /// </summary>\n        public ArchitectureCandidate<T> GetBestArchitecture()\n        {\n            return topArchitectures.FirstOrDefault() ?? new ArchitectureCandidate<T>();\n        }\n        \n        /// <summary>\n        /// Get top N architectures\n        /// </summary>\n        public List<ArchitectureCandidate<T>> GetTopArchitectures(int n = 5)\n        {\n            return topArchitectures.Take(n).ToList();\n        }\n        \n        private T ComputeFitnessScore(T accuracy, int parameters, long flops)\n        {\n            // Multi-objective fitness: accuracy vs efficiency\n            var ops = MathHelper.GetNumericOperations<T>();\n            var accuracyWeight = ops.FromDouble(0.7);\n            var efficiencyWeight = ops.FromDouble(0.3);\n            \n            // Normalize efficiency (fewer parameters and FLOPs is better)\n            var parametersLog = Math.Log10(parameters + 1);\n            var flopsLog = Math.Log10(flops + 1) / 10;\n            var efficiencyScore = ops.FromDouble(1.0 / (1.0 + parametersLog + flopsLog));\n            \n            return ops.Add(\n                ops.Multiply(accuracyWeight, accuracy), \n                ops.Multiply(efficiencyWeight, efficiencyScore)\n            );\n        }\n        \n        private int CountParameters(INeuralNetworkModel<T> network)\n        {\n            // Count total trainable parameters\n            return 1000000; // Placeholder\n        }\n        \n        private long EstimateFLOPs(INeuralNetworkModel<T> network, int[] inputShape)\n        {\n            // Estimate floating point operations\n            return 1000000000; // Placeholder\n        }\n        \n        private T TrainEpoch(INeuralNetworkModel<T> network, Tensor<T> data, Tensor<T> labels, AdamOptimizer<T, Tensor<T>, Tensor<T>> optimizer)\n        {\n            // Train one epoch and return loss\n            var ops = MathHelper.GetNumericOperations<T>();\n            return ops.FromDouble(0.1); // Placeholder\n        }\n        \n        private T EvaluateAccuracy(INeuralNetworkModel<T> network, Tensor<T> data, Tensor<T> labels)\n        {\n            // Evaluate accuracy on dataset\n            var ops = MathHelper.GetNumericOperations<T>();\n            return ops.FromDouble(0.9); // Placeholder\n        }\n        \n        private ArchitectureCandidate<T> CreateCandidateFromArchitecture(Architecture<T> architecture)\n        {\n            // Convert architecture representation to candidate\n            return new ArchitectureCandidate<T>(); // Placeholder\n        }\n        \n        private ArchitectureCandidate<T> CreateCandidateFromHyperparameters(Dictionary<string, object> hyperparameters)\n        {\n            // Convert hyperparameters to candidate\n            return new ArchitectureCandidate<T>(); // Placeholder\n        }\n        \n        private void LogProgress(string message)\n        {\n            Console.WriteLine($\"[NAS] {message}\");\n        }\n        \n        private void LogError(string message)\n        {\n            Console.WriteLine($\"[NAS ERROR] {message}\");\n        }\n\n        protected override Task<IFullModel<T, Tensor<T>, Tensor<T>>> CreateModelAsync(ModelType modelType, Dictionary<string, object> parameters)\n        {\n            throw new NotImplementedException(\"Neural Architecture<T> Search creates architectures, not individual models\");\n        }\n\n        protected override Dictionary<string, ParameterRange> GetDefaultSearchSpace(ModelType modelType)\n        {\n            return new Dictionary<string, ParameterRange>();\n        }\n    }\n\n    /// <summary>\n    /// Wrapper model for Neural Architecture Search results\n    /// </summary>\n    /// <typeparam name=\"T\">The numeric type used for calculations</typeparam>\n    public class NeuralArchitectureSearchModel<T> : IFullModel<T, Tensor<T>, Tensor<T>>\n    {\n        private readonly INeuralNetworkModel<T> _innerModel = default!;\n        private readonly ArchitectureCandidate<T> _architecture = default!;\n\n        public NeuralArchitectureSearchModel(INeuralNetworkModel<T> innerModel, ArchitectureCandidate<T> architecture)\n        {\n            _innerModel = innerModel;\n            _architecture = architecture;\n        }\n\n        public ModelType Type => ModelType.NeuralNetwork;\n\n        public string[] FeatureNames { get; set; } = Array.Empty<string>();\n\n        public int ParameterCount => _architecture.Parameters;\n\n        public void Train(Tensor<T> input, Tensor<T> expectedOutput)\n        {\n            // Training is handled during the search process\n            throw new NotSupportedException(\"NAS models are trained during the search process. Use SearchAsync instead.\");\n        }\n\n        public Tensor<T> Predict(Tensor<T> input)\n        {\n            // This is a simplified placeholder - actual implementation would depend on neural network predict method\n            throw new NotImplementedException(\"Prediction needs to be implemented based on the inner neural network model\");\n        }\n\n        public ModelMetaData<T> GetModelMetaData()\n        {\n            return new ModelMetaData<T>\n            {\n                Name = \"NeuralArchitectureSearch\",\n                Description = $\"Neural Architecture with {_architecture.Layers.Count} layers\",\n                Version = \"1.0\",\n                TrainingDate = DateTime.UtcNow,\n                Properties = new Dictionary<string, object>\n                {\n                    [\"Architecture\"] = _architecture,\n                    [\"Fitness\"] = _architecture.Fitness,\n                    [\"ValidationAccuracy\"] = _architecture.ValidationAccuracy,\n                    [\"Parameters\"] = _architecture.Parameters,\n                    [\"FLOPs\"] = _architecture.FLOPs,\n                    [\"NumLayers\"] = _architecture.Layers.Count\n                }\n            };\n        }\n\n        public void SaveModel(string filePath)\n        {\n            throw new NotImplementedException(\"Model serialization not yet implemented for NAS models\");\n        }\n\n        public void LoadModel(string filePath)\n        {\n            throw new NotImplementedException(\"Model deserialization not yet implemented for NAS models\");\n        }\n\n        public byte[] Serialize()\n        {\n            throw new NotImplementedException(\"Model serialization not yet implemented for NAS models\");\n        }\n\n        public void Deserialize(byte[] data)\n        {\n            throw new NotImplementedException(\"Model deserialization not yet implemented for NAS models\");\n        }\n\n        public Vector<T> GetParameters()\n        {\n            throw new NotImplementedException(\"GetParameters not yet implemented for NAS models\");\n        }\n\n        public void SetParameters(Vector<T> parameters)\n        {\n            throw new NotImplementedException(\"SetParameters not yet implemented for NAS models\");\n        }\n\n        public IFullModel<T, Tensor<T>, Tensor<T>> WithParameters(Vector<T> parameters)\n        {\n            throw new NotImplementedException(\"WithParameters not yet implemented for NAS models\");\n        }\n\n        public Dictionary<string, T> GetFeatureImportance()\n        {\n            // Neural networks typically don't have explicit feature importance\n            // Return empty dictionary for consistency with interface\n            return new Dictionary<string, T>();\n        }\n\n        public IEnumerable<int> GetActiveFeatureIndices()\n        {\n            // All features are typically active in neural networks\n            return Enumerable.Empty<int>();\n        }\n\n        public bool IsFeatureUsed(int featureIndex)\n        {\n            // All features are typically used in neural networks\n            return true;\n        }\n\n        public void SetActiveFeatureIndices(IEnumerable<int> featureIndices)\n        {\n            // Feature selection not applicable for neural networks\n        }\n\n        public IFullModel<T, Tensor<T>, Tensor<T>> Clone()\n        {\n            return new NeuralArchitectureSearchModel<T>(_innerModel, _architecture.Clone());\n        }\n\n        public IFullModel<T, Tensor<T>, Tensor<T>> DeepCopy()\n        {\n            return Clone();\n        }\n    }\n}",
  "unit_tests": "using Xunit;\nusing AiDotNet.AutoML;\nusing AiDotNet.LinearAlgebra;\nusing AiDotNet.Enums;\nusing System;\nusing System.Threading.Tasks;\n\nnamespace AiDotNet.Tests.AutoML\n{\n    public class NeuralArchitectureSearchTests\n    {\n        [Fact]\n        public void Constructor_ShouldInitializeWithDefaults()\n        {\n            // Arrange & Act\n            var nas = new NeuralArchitectureSearch<double>();\n\n            // Assert\n            Assert.NotNull(nas);\n            Assert.Equal(AutoMLStatus.NotStarted, nas.Status);\n        }\n\n        [Fact]\n        public void Constructor_ShouldAcceptCustomParameters()\n        {\n            // Arrange & Act\n            var nas = new NeuralArchitectureSearch<double>(\n                strategy: NeuralArchitectureSearchStrategy.RandomSearch,\n                maxLayers: 5,\n                maxNeuronsPerLayer: 256,\n                resourceBudget: 50.0,\n                populationSize: 30,\n                generations: 10\n            );\n\n            // Assert\n            Assert.NotNull(nas);\n        }\n\n        [Fact]\n        public async Task SuggestNextTrialAsync_ShouldReturnValidParameters()\n        {\n            // Arrange\n            var nas = new NeuralArchitectureSearch<double>();\n\n            // Act\n            var parameters = await nas.SuggestNextTrialAsync();\n\n            // Assert\n            Assert.NotNull(parameters);\n            Assert.True(parameters.ContainsKey(\"num_layers\"));\n            Assert.True(parameters.ContainsKey(\"layer_types\"));\n            Assert.True(parameters.ContainsKey(\"activation\"));\n            Assert.True(parameters.ContainsKey(\"dropout_rate\"));\n            Assert.True(parameters.ContainsKey(\"neurons_per_layer\"));\n        }\n\n        [Fact]\n        public void GetBestArchitecture_ShouldReturnEmptyWhenNoSearchPerformed()\n        {\n            // Arrange\n            var nas = new NeuralArchitectureSearch<double>();\n\n            // Act\n            var bestArch = nas.GetBestArchitecture();\n\n            // Assert\n            Assert.NotNull(bestArch);\n            Assert.Empty(bestArch.Layers);\n        }\n\n        [Fact]\n        public void GetTopArchitectures_ShouldReturnEmptyWhenNoSearchPerformed()\n        {\n            // Arrange\n            var nas = new NeuralArchitectureSearch<double>();\n\n            // Act\n            var topArchs = nas.GetTopArchitectures(5);\n\n            // Assert\n            Assert.NotNull(topArchs);\n            Assert.Empty(topArchs);\n        }\n\n        [Fact]\n        public void NeuralArchitectureSearchModel_ShouldInitializeCorrectly()\n        {\n            // Arrange\n            var architecture = new ArchitectureCandidate<double>();\n            architecture.Parameters = 1000;\n            architecture.Layers.Add(new LayerConfiguration<double> \n            { \n                Type = LayerType.FullyConnected,\n                Units = 128 \n            });\n\n            // Act\n            var model = new NeuralArchitectureSearchModel<double>(null!, architecture);\n\n            // Assert\n            Assert.NotNull(model);\n            Assert.Equal(ModelType.NeuralNetwork, model.Type);\n            Assert.Equal(1000, model.ParameterCount);\n        }\n\n        [Fact]\n        public void NeuralArchitectureSearchModel_Train_ShouldThrowNotSupportedException()\n        {\n            // Arrange\n            var architecture = new ArchitectureCandidate<double>();\n            var model = new NeuralArchitectureSearchModel<double>(null!, architecture);\n            var input = new Tensor<double>(new[] { 1, 10 });\n            var output = new Tensor<double>(new[] { 1, 1 });\n\n            // Act & Assert\n            Assert.Throws<NotSupportedException>(() => model.Train(input, output));\n        }\n\n        [Fact]\n        public void NeuralArchitectureSearchModel_Clone_ShouldCreateCopy()\n        {\n            // Arrange\n            var architecture = new ArchitectureCandidate<double>();\n            architecture.Parameters = 500;\n            var model = new NeuralArchitectureSearchModel<double>(null!, architecture);\n\n            // Act\n            var clone = model.Clone();\n\n            // Assert\n            Assert.NotNull(clone);\n            Assert.NotSame(model, clone);\n            Assert.Equal(model.ParameterCount, clone.ParameterCount);\n        }\n\n        [Fact]\n        public void NeuralArchitectureSearchModel_GetFeatureImportance_ShouldReturnEmptyDictionary()\n        {\n            // Arrange\n            var architecture = new ArchitectureCandidate<double>();\n            var model = new NeuralArchitectureSearchModel<double>(null!, architecture);\n\n            // Act\n            var importance = model.GetFeatureImportance();\n\n            // Assert\n            Assert.NotNull(importance);\n            Assert.Empty(importance);\n        }\n\n        [Fact]\n        public void NeuralArchitectureSearchModel_GetModelMetaData_ShouldReturnValidMetadata()\n        {\n            // Arrange\n            var architecture = new ArchitectureCandidate<double>();\n            architecture.Layers.Add(new LayerConfiguration<double> \n            { \n                Type = LayerType.FullyConnected,\n                Units = 64 \n            });\n            architecture.Parameters = 2000;\n            architecture.FLOPs = 1000000;\n            var model = new NeuralArchitectureSearchModel<double>(null!, architecture);\n\n            // Act\n            var metadata = model.GetModelMetaData();\n\n            // Assert\n            Assert.NotNull(metadata);\n            Assert.Equal(\"NeuralArchitectureSearch\", metadata.Name);\n            Assert.Equal(\"1.0\", metadata.Version);\n            Assert.NotNull(metadata.Properties);\n            Assert.True(metadata.Properties.ContainsKey(\"Architecture\"));\n            Assert.True(metadata.Properties.ContainsKey(\"Parameters\"));\n            Assert.True(metadata.Properties.ContainsKey(\"FLOPs\"));\n            Assert.True(metadata.Properties.ContainsKey(\"NumLayers\"));\n        }\n    }\n}",
  "summary": "The file has multiple critical build errors that prevent compilation. The main issues are: (1) unclosed multi-line comment block at line 382, (2) missing closing brace for RunGradientBasedSearch method, (3) duplicate using statement, (4) duplicate SearchAsync method, (5) duplicate SuggestNextTrialAsync method, and (6) type mismatches in NeuralArchitectureSearchModel using non-generic types instead of generic type T.",
  "impact": "HIGH - These are blocking compilation errors that prevent the entire project from building",
  "testing_notes": "After applying fixes, verify: (1) File compiles without errors, (2) No duplicate method errors, (3) Generic types are correctly used throughout, (4) All unit tests pass"
}
