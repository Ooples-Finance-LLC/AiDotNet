{
  "bug_id": "BUG-011",
  "title": "Fix and Productionize MultiHeadAttention Layer",
  "description": "Implement correct reshaping, softmax, backward pass for MultiHeadAttention layer",
  "severity": "CRITICAL",
  "affected_files": [
    "C:\\Users\\cheat\\source\\repos\\AiDotNet\\src\\NeuralNetworks\\Layers\\MultiHeadAttention.cs",
    "C:\\Users\\cheat\\source\\repos\\AiDotNet\\src\\NeuralNetworks\\Layers\\MultiHeadAttentionLayer.cs"
  ],
  "analysis": {
    "root_causes": [
      "ReshapeForAttention returns input unchanged - needs to reshape [batch, seq, embed] to [batch, heads, seq, head_dim]",
      "ReshapeFromAttention returns input unchanged - needs to reshape [batch, heads, seq, head_dim] back to [batch, seq, embed]",
      "Softmax returns input unchanged - needs numerically stable softmax with max subtraction",
      "Backward pass is oversimplified - doesn't properly compute gradients through attention mechanism",
      "Missing gradient caching for attention scores, queries, keys, values needed for backward pass"
    ],
    "impact": [
      "Attention mechanism completely non-functional in MultiHeadAttention.cs",
      "Training gradients are incorrect, preventing learning",
      "Numerical instability in softmax can cause NaN/Inf values",
      "MultiHeadAttentionLayer.cs has better implementation but backward pass may have issues"
    ]
  },
  "fixes": [
    {
      "method": "ReshapeForAttention",
      "file": "MultiHeadAttention.cs",
      "line": 152,
      "description": "Reshape tensor from [batch, seq_len, embed_dim] to [batch, num_heads, seq_len, head_dim]",
      "current_code": "private Tensor<T> ReshapeForAttention(Tensor<T> x, int batchSize, int seqLen)\n{\n    // Reshape to multi-head format\n    // Simplified implementation\n    return x;\n}",
      "fixed_code": "/// <summary>\n/// Reshapes tensor for multi-head attention processing.\n/// Transforms [batch, seq_len, embed_dim] to [batch, num_heads, seq_len, head_dim]\n/// </summary>\n/// <param name=\"x\">Input tensor of shape [batch, seq_len, embed_dim]</param>\n/// <param name=\"batchSize\">Batch size</param>\n/// <param name=\"seqLen\">Sequence length</param>\n/// <returns>Reshaped tensor of shape [batch, num_heads, seq_len, head_dim]</returns>\n/// <remarks>\n/// <para>\n/// <b>For Beginners:</b> Multi-head attention splits the embedding dimension into multiple \"heads\".\n/// Each head processes the sequence independently, allowing the model to attend to different aspects\n/// of the input simultaneously. This reshaping operation organizes the data so each head has its own\n/// dedicated portion of the embedding dimension.\n/// </para>\n/// <para>\n/// Example: If embed_dim=512 and num_heads=8, then head_dim=64.\n/// Input shape: [2, 10, 512] (batch=2, seq_len=10, embed=512)\n/// Output shape: [2, 8, 10, 64] (batch=2, heads=8, seq_len=10, head_dim=64)\n/// </para>\n/// </remarks>\nprivate Tensor<T> ReshapeForAttention(Tensor<T> x, int batchSize, int seqLen)\n{\n    // Input: [batch_size, seq_len, embed_dim]\n    // Step 1: Reshape to [batch_size, seq_len, num_heads, head_dim]\n    var reshaped = x.Reshape(batchSize, seqLen, numHeads, headDim);\n    \n    // Step 2: Transpose to [batch_size, num_heads, seq_len, head_dim]\n    // This groups all data for each head together\n    var transposed = reshaped.Transpose(new[] { 0, 2, 1, 3 });\n    \n    return transposed;\n}",
      "validation": "Shape should be [batch_size, num_heads, seq_len, head_dim]"
    },
    {
      "method": "ReshapeFromAttention",
      "file": "MultiHeadAttention.cs",
      "line": 159,
      "description": "Reshape tensor from [batch, num_heads, seq_len, head_dim] back to [batch, seq_len, embed_dim]",
      "current_code": "private Tensor<T> ReshapeFromAttention(Tensor<T> x, int batchSize, int seqLen)\n{\n    // Reshape from multi-head format\n    // Simplified implementation\n    return x;\n}",
      "fixed_code": "/// <summary>\n/// Reshapes tensor from multi-head attention format back to standard format.\n/// Transforms [batch, num_heads, seq_len, head_dim] to [batch, seq_len, embed_dim]\n/// </summary>\n/// <param name=\"x\">Input tensor of shape [batch, num_heads, seq_len, head_dim]</param>\n/// <param name=\"batchSize\">Batch size</param>\n/// <param name=\"seqLen\">Sequence length</param>\n/// <returns>Reshaped tensor of shape [batch, seq_len, embed_dim]</returns>\n/// <remarks>\n/// <para>\n/// <b>For Beginners:</b> After all attention heads have processed the sequence independently,\n/// we need to combine their outputs back into a single tensor. This is the inverse operation\n/// of ReshapeForAttention - it concatenates all head outputs back into the original embedding dimension.\n/// </para>\n/// <para>\n/// Example: If we have [2, 8, 10, 64] (batch=2, heads=8, seq_len=10, head_dim=64)\n/// Output: [2, 10, 512] (batch=2, seq_len=10, embed=512 where 512=8*64)\n/// </para>\n/// </remarks>\nprivate Tensor<T> ReshapeFromAttention(Tensor<T> x, int batchSize, int seqLen)\n{\n    // Input: [batch_size, num_heads, seq_len, head_dim]\n    // Step 1: Transpose to [batch_size, seq_len, num_heads, head_dim]\n    var transposed = x.Transpose(new[] { 0, 2, 1, 3 });\n    \n    // Step 2: Reshape to [batch_size, seq_len, embed_dim]\n    // This concatenates all head outputs along the embedding dimension\n    var reshaped = transposed.Reshape(batchSize, seqLen, embedDim);\n    \n    return reshaped;\n}",
      "validation": "Shape should be [batch_size, seq_len, embed_dim]"
    },
    {
      "method": "Softmax",
      "file": "MultiHeadAttention.cs",
      "line": 173,
      "description": "Implement numerically stable softmax along the last dimension",
      "current_code": "private Tensor<T> Softmax(Tensor<T> scores)\n{\n    // Apply softmax to attention scores\n    // Simplified implementation\n    return scores;\n}",
      "fixed_code": "/// <summary>\n/// Applies numerically stable softmax activation to attention scores.\n/// Operates along the last dimension (keys dimension) for each query.\n/// </summary>\n/// <param name=\"scores\">Attention scores of shape [batch, num_heads, seq_len_q, seq_len_k]</param>\n/// <returns>Attention weights (probabilities) of same shape, summing to 1 along last dimension</returns>\n/// <remarks>\n/// <para>\n/// <b>For Beginners:</b> Softmax converts raw attention scores into probabilities (0 to 1, summing to 1).\n/// These probabilities determine how much each position in the input should \"attend to\" other positions.\n/// </para>\n/// <para>\n/// <b>Numerical Stability:</b> We subtract the maximum value before computing exponentials to prevent\n/// overflow. The mathematical result is identical, but numerically much more stable.\n/// Without this, exp(large_number) can produce infinity or NaN values.\n/// </para>\n/// <para>\n/// Example: Raw scores [10.5, 8.2, 9.1] become probabilities [0.82, 0.03, 0.15]\n/// The highest score (10.5) gets the highest probability (0.82).\n/// </para>\n/// </remarks>\nprivate Tensor<T> Softmax(Tensor<T> scores)\n{\n    // scores shape: [batch_size, num_heads, seq_len_query, seq_len_key]\n    // Apply softmax along the last dimension (seq_len_key)\n    \n    var shape = scores.Shape;\n    int batchSize = shape[0];\n    int numHeads = shape[1];\n    int seqLenQ = shape[2];\n    int seqLenK = shape[3];\n    \n    var result = new Tensor<T>(shape);\n    \n    // Process each (batch, head, query) position independently\n    for (int b = 0; b < batchSize; b++)\n    {\n        for (int h = 0; h < numHeads; h++)\n        {\n            for (int q = 0; q < seqLenQ; q++)\n            {\n                // Extract scores for this query position\n                // Find max for numerical stability\n                T maxScore = scores[b, h, q, 0];\n                for (int k = 0; k < seqLenK; k++)\n                {\n                    T score = scores[b, h, q, k];\n                    if (NumOps.GreaterThan(score, maxScore))\n                    {\n                        maxScore = score;\n                    }\n                }\n                \n                // Compute exp(score - max_score) and sum\n                T sumExp = NumOps.Zero;\n                var expScores = new T[seqLenK];\n                for (int k = 0; k < seqLenK; k++)\n                {\n                    T score = scores[b, h, q, k];\n                    T shiftedScore = NumOps.Subtract(score, maxScore);\n                    T expScore = NumOps.Exp(shiftedScore);\n                    expScores[k] = expScore;\n                    sumExp = NumOps.Add(sumExp, expScore);\n                }\n                \n                // Normalize to get probabilities\n                for (int k = 0; k < seqLenK; k++)\n                {\n                    result[b, h, q, k] = NumOps.Divide(expScores[k], sumExp);\n                }\n            }\n        }\n    }\n    \n    return result;\n}",
      "validation": "Each row should sum to 1.0 along last dimension, all values in [0,1]"
    },
    {
      "method": "Backward",
      "file": "MultiHeadAttention.cs",
      "line": 75,
      "description": "Implement complete backward pass through attention mechanism with proper gradient flow",
      "current_code": "public override Tensor<T> Backward(Tensor<T> gradOutput)\n{\n    // Simplified backward pass\n    var gradOut = outProj.Backward(gradOutput);\n    var gradQ = queryProj.Backward(gradOut);\n    var gradK = keyProj.Backward(gradOut);\n    var gradV = valueProj.Backward(gradOut);\n    \n    return gradQ.Add(gradK).Add(gradV);\n}",
      "fixed_code": "// ADD THESE FIELDS TO THE CLASS (after line 22):\nprivate Tensor<T>? cachedInput;\nprivate Tensor<T>? cachedQ;\nprivate Tensor<T>? cachedK;\nprivate Tensor<T>? cachedV;\nprivate Tensor<T>? cachedQReshaped;\nprivate Tensor<T>? cachedKReshaped;\nprivate Tensor<T>? cachedVReshaped;\nprivate Tensor<T>? cachedAttentionScores;\nprivate Tensor<T>? cachedAttentionWeights;\nprivate Tensor<T>? cachedAttentionOutput;\n\n// MODIFY Forward METHOD to cache intermediate values (after line 43):\npublic Tensor<T> Forward(Tensor<T> query, Tensor<T> key, Tensor<T> value)\n{\n    cachedInput = query; // Cache for backward pass\n    var batchSize = query.Shape[0];\n    var seqLen = query.Shape[1];\n    \n    // Project and cache\n    cachedQ = queryProj.Forward(query);\n    cachedK = keyProj.Forward(key);\n    cachedV = valueProj.Forward(value);\n    \n    // Reshape to [batch, heads, seq_len, head_dim] and cache\n    cachedQReshaped = ReshapeForAttention(cachedQ, batchSize, seqLen);\n    cachedKReshaped = ReshapeForAttention(cachedK, batchSize, seqLen);\n    cachedVReshaped = ReshapeForAttention(cachedV, batchSize, seqLen);\n    \n    // Scaled dot-product attention\n    cachedAttentionScores = ComputeAttentionScores(cachedQReshaped, cachedKReshaped);\n    cachedAttentionWeights = Softmax(cachedAttentionScores);\n    cachedAttentionOutput = ApplyAttention(cachedAttentionWeights, cachedVReshaped);\n    \n    // Reshape back and project\n    var attnOutputReshaped = ReshapeFromAttention(cachedAttentionOutput, batchSize, seqLen);\n    var output = outProj.Forward(attnOutputReshaped);\n    \n    return output;\n}\n\n// COMPLETE BACKWARD PASS:\n/// <summary>\n/// Performs backward pass through multi-head attention mechanism.\n/// Computes gradients with respect to input and all parameters.\n/// </summary>\n/// <param name=\"gradOutput\">Gradient from next layer of shape [batch, seq_len, embed_dim]</param>\n/// <returns>Gradient with respect to input of shape [batch, seq_len, embed_dim]</returns>\n/// <remarks>\n/// <para>\n/// <b>For Beginners:</b> The backward pass is where the network learns. It calculates how much\n/// each input and parameter contributed to the final output, allowing the network to adjust them\n/// to reduce errors. This is the most complex part of attention because gradients must flow\n/// through multiple transformations: projections, reshaping, softmax, and matrix multiplications.\n/// </para>\n/// <para>\n/// The backward pass follows the reverse order of the forward pass:\n/// 1. Gradient through output projection\n/// 2. Gradient through reshape (from attention format)\n/// 3. Gradient through attention application (weights @ values)\n/// 4. Gradient through softmax\n/// 5. Gradient through attention scores computation (Q @ K^T)\n/// 6. Gradient through reshape (to attention format)\n/// 7. Gradient through Q, K, V projections\n/// </para>\n/// </remarks>\npublic override Tensor<T> Backward(Tensor<T> gradOutput)\n{\n    if (cachedInput == null || cachedQ == null || cachedK == null || cachedV == null ||\n        cachedQReshaped == null || cachedKReshaped == null || cachedVReshaped == null ||\n        cachedAttentionScores == null || cachedAttentionWeights == null || cachedAttentionOutput == null)\n    {\n        throw new InvalidOperationException(\"Forward pass must be called before backward pass\");\n    }\n    \n    var batchSize = gradOutput.Shape[0];\n    var seqLen = gradOutput.Shape[1];\n    \n    // Step 1: Backward through output projection\n    var gradAttnOutputReshaped = outProj.Backward(gradOutput);\n    \n    // Step 2: Backward through reshape (from attention format)\n    // gradAttnOutputReshaped: [batch, seq_len, embed_dim]\n    // Need to reshape to [batch, num_heads, seq_len, head_dim]\n    var gradAttnOutput = gradAttnOutputReshaped.Reshape(batchSize, seqLen, numHeads, headDim)\n                                               .Transpose(new[] { 0, 2, 1, 3 });\n    \n    // Step 3: Backward through attention application (attn_output = attn_weights @ V)\n    // gradAttnOutput: [batch, heads, seq_len_q, head_dim]\n    // cachedAttentionWeights: [batch, heads, seq_len_q, seq_len_k]\n    // cachedVReshaped: [batch, heads, seq_len_k, head_dim]\n    \n    // Gradient w.r.t. attention weights: gradAttnOutput @ V^T\n    var gradAttnWeights = MatMulLastTwoDims(gradAttnOutput, TransposeLastTwoDims(cachedVReshaped));\n    \n    // Gradient w.r.t. values: attn_weights^T @ gradAttnOutput\n    var gradVReshaped = MatMulLastTwoDims(TransposeLastTwoDims(cachedAttentionWeights), gradAttnOutput);\n    \n    // Step 4: Backward through softmax\n    var gradAttnScores = SoftmaxBackward(cachedAttentionWeights, gradAttnWeights);\n    \n    // Step 5: Backward through scaled dot-product (scores = Q @ K^T / sqrt(head_dim))\n    var scale = NumOps.FromDouble(1.0 / Math.Sqrt(headDim));\n    gradAttnScores = gradAttnScores.Multiply(scale);\n    \n    // Gradient w.r.t. queries: gradAttnScores @ K\n    var gradQReshaped = MatMulLastTwoDims(gradAttnScores, cachedKReshaped);\n    \n    // Gradient w.r.t. keys: gradAttnScores^T @ Q\n    var gradKReshaped = MatMulLastTwoDims(TransposeLastTwoDims(gradAttnScores), cachedQReshaped);\n    \n    // Step 6: Backward through reshape (to attention format)\n    // Reshape gradients from [batch, heads, seq_len, head_dim] to [batch, seq_len, embed_dim]\n    var gradQ = ReshapeFromAttention(gradQReshaped, batchSize, seqLen);\n    var gradK = ReshapeFromAttention(gradKReshaped, batchSize, seqLen);\n    var gradV = ReshapeFromAttention(gradVReshaped, batchSize, seqLen);\n    \n    // Step 7: Backward through Q, K, V projections\n    var gradInputFromQ = queryProj.Backward(gradQ);\n    var gradInputFromK = keyProj.Backward(gradK);\n    var gradInputFromV = valueProj.Backward(gradV);\n    \n    // Sum gradients from all three paths\n    return gradInputFromQ.Add(gradInputFromK).Add(gradInputFromV);\n}\n\n/// <summary>\n/// Computes gradient through softmax activation.\n/// </summary>\n/// <param name=\"softmaxOutput\">The output of softmax (probabilities)</param>\n/// <param name=\"gradOutput\">Gradient flowing back from next layer</param>\n/// <returns>Gradient with respect to softmax input</returns>\nprivate Tensor<T> SoftmaxBackward(Tensor<T> softmaxOutput, Tensor<T> gradOutput)\n{\n    // Softmax backward: grad_input[i] = softmax[i] * (grad_output[i] - sum(softmax * grad_output))\n    var shape = softmaxOutput.Shape;\n    int batchSize = shape[0];\n    int numHeads = shape[1];\n    int seqLenQ = shape[2];\n    int seqLenK = shape[3];\n    \n    var result = new Tensor<T>(shape);\n    \n    for (int b = 0; b < batchSize; b++)\n    {\n        for (int h = 0; h < numHeads; h++)\n        {\n            for (int q = 0; q < seqLenQ; q++)\n            {\n                // Compute sum(softmax * grad_output) for this row\n                T sumSoftmaxGrad = NumOps.Zero;\n                for (int k = 0; k < seqLenK; k++)\n                {\n                    T softmax_val = softmaxOutput[b, h, q, k];\n                    T grad_val = gradOutput[b, h, q, k];\n                    sumSoftmaxGrad = NumOps.Add(sumSoftmaxGrad, NumOps.Multiply(softmax_val, grad_val));\n                }\n                \n                // Compute gradient for each position\n                for (int k = 0; k < seqLenK; k++)\n                {\n                    T softmax_val = softmaxOutput[b, h, q, k];\n                    T grad_val = gradOutput[b, h, q, k];\n                    T grad_input = NumOps.Multiply(softmax_val, NumOps.Subtract(grad_val, sumSoftmaxGrad));\n                    result[b, h, q, k] = grad_input;\n                }\n            }\n        }\n    }\n    \n    return result;\n}\n\n/// <summary>\n/// Matrix multiplication along last two dimensions of 4D tensors.\n/// </summary>\nprivate Tensor<T> MatMulLastTwoDims(Tensor<T> a, Tensor<T> b)\n{\n    // a: [batch, heads, m, n]\n    // b: [batch, heads, n, p]\n    // result: [batch, heads, m, p]\n    \n    int batchSize = a.Shape[0];\n    int numHeads = a.Shape[1];\n    int m = a.Shape[2];\n    int n = a.Shape[3];\n    int p = b.Shape[3];\n    \n    var result = new Tensor<T>(new[] { batchSize, numHeads, m, p });\n    \n    for (int batch = 0; batch < batchSize; batch++)\n    {\n        for (int head = 0; head < numHeads; head++)\n        {\n            for (int i = 0; i < m; i++)\n            {\n                for (int j = 0; j < p; j++)\n                {\n                    T sum = NumOps.Zero;\n                    for (int k = 0; k < n; k++)\n                    {\n                        T aVal = a[batch, head, i, k];\n                        T bVal = b[batch, head, k, j];\n                        sum = NumOps.Add(sum, NumOps.Multiply(aVal, bVal));\n                    }\n                    result[batch, head, i, j] = sum;\n                }\n            }\n        }\n    }\n    \n    return result;\n}\n\n/// <summary>\n/// Transposes last two dimensions of a 4D tensor.\n/// </summary>\nprivate Tensor<T> TransposeLastTwoDims(Tensor<T> x)\n{\n    // Input: [batch, heads, dim1, dim2]\n    // Output: [batch, heads, dim2, dim1]\n    return x.Transpose(new[] { 0, 1, 3, 2 });\n}",
      "validation": "Gradient shapes must match input shapes, numerical gradient checking should pass"
    },
    {
      "method": "ResetState",
      "file": "MultiHeadAttention.cs",
      "line": 194,
      "description": "Clear cached tensors when resetting state",
      "current_code": "public override void ResetState()\n{\n    queryProj.ResetState();\n    keyProj.ResetState();\n    valueProj.ResetState();\n    outProj.ResetState();\n}",
      "fixed_code": "/// <summary>\n/// Resets the internal state of the multi-head attention layer.\n/// Clears all cached values from forward and backward passes.\n/// </summary>\npublic override void ResetState()\n{\n    // Clear cached tensors\n    cachedInput = null;\n    cachedQ = null;\n    cachedK = null;\n    cachedV = null;\n    cachedQReshaped = null;\n    cachedKReshaped = null;\n    cachedVReshaped = null;\n    cachedAttentionScores = null;\n    cachedAttentionWeights = null;\n    cachedAttentionOutput = null;\n    \n    // Reset sub-layers\n    queryProj.ResetState();\n    keyProj.ResetState();\n    valueProj.ResetState();\n    outProj.ResetState();\n}",
      "validation": "All cached tensors should be null after reset"
    }
  ],
  "testing_requirements": {
    "unit_tests": [
      {
        "test_name": "Test_ReshapeForAttention_CorrectShape",
        "description": "Verify reshape produces [batch, heads, seq, head_dim]",
        "code": "[Fact]\npublic void Test_ReshapeForAttention_CorrectShape()\n{\n    // Arrange\n    int batchSize = 2;\n    int seqLen = 10;\n    int embedDim = 512;\n    int numHeads = 8;\n    int headDim = embedDim / numHeads; // 64\n    \n    var layer = new MultiHeadAttention<double>(embedDim, numHeads, 0.0);\n    var input = new Tensor<double>(new[] { batchSize, seqLen, embedDim });\n    \n    // Fill with test data\n    for (int i = 0; i < input.Length; i++)\n    {\n        input[i] = i * 0.01;\n    }\n    \n    // Act\n    var reshapeMethod = typeof(MultiHeadAttention<double>)\n        .GetMethod(\"ReshapeForAttention\", System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);\n    var result = (Tensor<double>)reshapeMethod.Invoke(layer, new object[] { input, batchSize, seqLen });\n    \n    // Assert\n    Assert.Equal(4, result.Rank);\n    Assert.Equal(batchSize, result.Shape[0]);\n    Assert.Equal(numHeads, result.Shape[1]);\n    Assert.Equal(seqLen, result.Shape[2]);\n    Assert.Equal(headDim, result.Shape[3]);\n}"
      },
      {
        "test_name": "Test_ReshapeFromAttention_InverseOfReshapeFor",
        "description": "Verify reshape operations are inverses of each other",
        "code": "[Fact]\npublic void Test_ReshapeFromAttention_InverseOfReshapeFor()\n{\n    // Arrange\n    int batchSize = 2;\n    int seqLen = 10;\n    int embedDim = 512;\n    int numHeads = 8;\n    \n    var layer = new MultiHeadAttention<double>(embedDim, numHeads, 0.0);\n    var input = new Tensor<double>(new[] { batchSize, seqLen, embedDim });\n    \n    // Fill with sequential values for easy verification\n    for (int i = 0; i < input.Length; i++)\n    {\n        input[i] = i;\n    }\n    \n    // Act\n    var reshapeForMethod = typeof(MultiHeadAttention<double>)\n        .GetMethod(\"ReshapeForAttention\", System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);\n    var reshapeFromMethod = typeof(MultiHeadAttention<double>)\n        .GetMethod(\"ReshapeFromAttention\", System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);\n    \n    var intermediate = (Tensor<double>)reshapeForMethod.Invoke(layer, new object[] { input, batchSize, seqLen });\n    var result = (Tensor<double>)reshapeFromMethod.Invoke(layer, new object[] { intermediate, batchSize, seqLen });\n    \n    // Assert\n    Assert.Equal(input.Shape, result.Shape);\n    for (int i = 0; i < input.Length; i++)\n    {\n        Assert.Equal(input[i], result[i], precision: 10);\n    }\n}"
      },
      {
        "test_name": "Test_Softmax_OutputsValidProbabilities",
        "description": "Verify softmax produces valid probabilities (sum to 1, all in [0,1])",
        "code": "[Fact]\npublic void Test_Softmax_OutputsValidProbabilities()\n{\n    // Arrange\n    int batchSize = 2;\n    int numHeads = 8;\n    int seqLen = 10;\n    \n    var layer = new MultiHeadAttention<double>(512, numHeads, 0.0);\n    var scores = new Tensor<double>(new[] { batchSize, numHeads, seqLen, seqLen });\n    \n    // Fill with random scores\n    var random = new Random(42);\n    for (int i = 0; i < scores.Length; i++)\n    {\n        scores[i] = random.NextDouble() * 10 - 5; // Random values in [-5, 5]\n    }\n    \n    // Act\n    var softmaxMethod = typeof(MultiHeadAttention<double>)\n        .GetMethod(\"Softmax\", System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);\n    var result = (Tensor<double>)softmaxMethod.Invoke(layer, new object[] { scores });\n    \n    // Assert\n    for (int b = 0; b < batchSize; b++)\n    {\n        for (int h = 0; h < numHeads; h++)\n        {\n            for (int q = 0; q < seqLen; q++)\n            {\n                // Check each row sums to 1\n                double sum = 0.0;\n                for (int k = 0; k < seqLen; k++)\n                {\n                    double val = result[b, h, q, k];\n                    // Check value is in [0, 1]\n                    Assert.True(val >= 0.0 && val <= 1.0, $\"Value {val} not in [0,1]\");\n                    sum += val;\n                }\n                // Check sum is approximately 1\n                Assert.Equal(1.0, sum, precision: 6);\n            }\n        }\n    }\n}"
      },
      {
        "test_name": "Test_Softmax_NumericalStability",
        "description": "Verify softmax handles large values without overflow",
        "code": "[Fact]\npublic void Test_Softmax_NumericalStability()\n{\n    // Arrange\n    int batchSize = 1;\n    int numHeads = 1;\n    int seqLen = 5;\n    \n    var layer = new MultiHeadAttention<double>(64, numHeads, 0.0);\n    var scores = new Tensor<double>(new[] { batchSize, numHeads, seqLen, seqLen });\n    \n    // Fill with very large values that would overflow without max subtraction\n    for (int i = 0; i < scores.Length; i++)\n    {\n        scores[i] = 1000.0; // Large value\n    }\n    scores[0, 0, 0, 0] = 1001.0; // One slightly larger\n    \n    // Act\n    var softmaxMethod = typeof(MultiHeadAttention<double>)\n        .GetMethod(\"Softmax\", System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);\n    var result = (Tensor<double>)softmaxMethod.Invoke(layer, new object[] { scores });\n    \n    // Assert - no NaN or Infinity values\n    for (int i = 0; i < result.Length; i++)\n    {\n        Assert.False(double.IsNaN(result[i]), \"Result contains NaN\");\n        Assert.False(double.IsInfinity(result[i]), \"Result contains Infinity\");\n    }\n    \n    // The first position should have highest probability\n    Assert.True(result[0, 0, 0, 0] > result[0, 0, 0, 1]);\n}"
      },
      {
        "test_name": "Test_ForwardBackward_ShapeConsistency",
        "description": "Verify forward and backward pass produce consistent shapes",
        "code": "[Fact]\npublic void Test_ForwardBackward_ShapeConsistency()\n{\n    // Arrange\n    int batchSize = 2;\n    int seqLen = 10;\n    int embedDim = 512;\n    int numHeads = 8;\n    \n    var layer = new MultiHeadAttention<double>(embedDim, numHeads, 0.0);\n    var input = new Tensor<double>(new[] { batchSize, seqLen, embedDim });\n    \n    // Fill with random data\n    var random = new Random(42);\n    for (int i = 0; i < input.Length; i++)\n    {\n        input[i] = random.NextDouble();\n    }\n    \n    // Act\n    var output = layer.Forward(input);\n    var gradOutput = new Tensor<double>(output.Shape);\n    for (int i = 0; i < gradOutput.Length; i++)\n    {\n        gradOutput[i] = random.NextDouble();\n    }\n    var gradInput = layer.Backward(gradOutput);\n    \n    // Assert\n    Assert.Equal(input.Shape, output.Shape);\n    Assert.Equal(input.Shape, gradInput.Shape);\n}"
      },
      {
        "test_name": "Test_GradientNumericalCheck",
        "description": "Verify gradients using numerical differentiation (critical test)",
        "code": "[Fact]\npublic void Test_GradientNumericalCheck()\n{\n    // Arrange\n    int batchSize = 1;\n    int seqLen = 3;\n    int embedDim = 16;\n    int numHeads = 2;\n    double epsilon = 1e-4;\n    double tolerance = 1e-3;\n    \n    var layer = new MultiHeadAttention<double>(embedDim, numHeads, 0.0);\n    var input = new Tensor<double>(new[] { batchSize, seqLen, embedDim });\n    \n    // Fill with small random values\n    var random = new Random(42);\n    for (int i = 0; i < input.Length; i++)\n    {\n        input[i] = random.NextDouble() * 0.1;\n    }\n    \n    // Forward pass\n    var output = layer.Forward(input);\n    \n    // Gradient of loss (just use 1s for simplicity)\n    var gradOutput = new Tensor<double>(output.Shape);\n    for (int i = 0; i < gradOutput.Length; i++)\n    {\n        gradOutput[i] = 1.0;\n    }\n    \n    // Backward pass\n    var gradInputAnalytic = layer.Backward(gradOutput);\n    \n    // Numerical gradient check (sample a few positions)\n    var numericGradient = new Tensor<double>(input.Shape);\n    for (int testIdx = 0; testIdx < Math.Min(10, input.Length); testIdx++)\n    {\n        // Perturb input slightly\n        double original = input[testIdx];\n        \n        input[testIdx] = original + epsilon;\n        var outputPlus = layer.Forward(input);\n        double lossPlus = 0;\n        for (int i = 0; i < outputPlus.Length; i++)\n        {\n            lossPlus += outputPlus[i] * gradOutput[i];\n        }\n        \n        input[testIdx] = original - epsilon;\n        var outputMinus = layer.Forward(input);\n        double lossMinus = 0;\n        for (int i = 0; i < outputMinus.Length; i++)\n        {\n            lossMinus += outputMinus[i] * gradOutput[i];\n        }\n        \n        input[testIdx] = original;\n        \n        // Numerical gradient\n        numericGradient[testIdx] = (lossPlus - lossMinus) / (2 * epsilon);\n        \n        // Compare with analytic gradient\n        double analyticGrad = gradInputAnalytic[testIdx];\n        double numericGrad = numericGradient[testIdx];\n        double relativeError = Math.Abs(analyticGrad - numericGrad) / (Math.Abs(analyticGrad) + Math.Abs(numericGrad) + 1e-8);\n        \n        Assert.True(relativeError < tolerance, \n            $\"Gradient mismatch at position {testIdx}: analytic={analyticGrad}, numeric={numericGrad}, relative_error={relativeError}\");\n    }\n}"
      },
      {
        "test_name": "Test_ParameterGradients_NonZero",
        "description": "Verify parameter gradients are computed and non-zero",
        "code": "[Fact]\npublic void Test_ParameterGradients_NonZero()\n{\n    // Arrange\n    int batchSize = 2;\n    int seqLen = 10;\n    int embedDim = 64;\n    int numHeads = 4;\n    \n    var layer = new MultiHeadAttention<double>(embedDim, numHeads, 0.0);\n    var input = new Tensor<double>(new[] { batchSize, seqLen, embedDim });\n    \n    var random = new Random(42);\n    for (int i = 0; i < input.Length; i++)\n    {\n        input[i] = random.NextDouble();\n    }\n    \n    // Act\n    var output = layer.Forward(input);\n    var gradOutput = new Tensor<double>(output.Shape);\n    for (int i = 0; i < gradOutput.Length; i++)\n    {\n        gradOutput[i] = random.NextDouble();\n    }\n    layer.Backward(gradOutput);\n    \n    // Assert - check that gradients were computed\n    var paramGradients = layer.GetParameterGradients();\n    Assert.True(paramGradients.Length > 0, \"No parameter gradients\");\n    \n    // Check that at least some gradients are non-zero\n    int nonZeroCount = 0;\n    for (int i = 0; i < paramGradients.Length; i++)\n    {\n        if (Math.Abs(paramGradients[i]) > 1e-10)\n        {\n            nonZeroCount++;\n        }\n    }\n    Assert.True(nonZeroCount > 0, \"All parameter gradients are zero\");\n}"
      }
    ],
    "integration_tests": [
      {
        "test_name": "Test_MultiHeadAttention_InTransformerBlock",
        "description": "Test attention layer as part of a transformer block",
        "notes": "Verify attention integrates correctly with LayerNorm and FFN"
      },
      {
        "test_name": "Test_Training_ConvergenceOnSimpleTask",
        "description": "Train a small transformer with attention on a simple sequence task",
        "notes": "Verify loss decreases over training iterations"
      }
    ]
  },
  "priority": "CRITICAL",
  "estimated_effort": "8 hours",
  "risk_assessment": {
    "complexity": "HIGH",
    "breaking_changes": true,
    "backwards_compatibility": "Breaking - cached fields added, forward method signature changed",
    "mitigation": "Comprehensive unit tests, gradient checking, numerical stability tests"
  },
  "notes": [
    "The backward pass is the most complex part - requires careful handling of gradient flow through softmax and matrix multiplications",
    "Numerical stability in softmax is CRITICAL - max subtraction prevents overflow/underflow",
    "All intermediate values must be cached during forward pass for use in backward pass",
    "Gradient checking (numerical vs analytic) is essential to verify correctness",
    "MultiHeadAttentionLayer.cs has better structure but may also need backward pass review",
    "Consider consolidating the two implementations into one production-ready version",
    "Attention mask support should be added in future iterations",
    "Dropout during training should be implemented for regularization"
  ],
  "references": [
    "Attention Is All You Need - Vaswani et al. 2017 (https://arxiv.org/abs/1706.03762)",
    "The Illustrated Transformer - Jay Alammar (https://jalammar.github.io/illustrated-transformer/)",
    "PyTorch MultiheadAttention Implementation (https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)"
  ]
}
